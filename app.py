import gradio as gr
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer
import time
import psutil
import GPUtil
import os
from datetime import datetime
import threading
import re
import gc
import json
import uuid
import shutil
from pathlib import Path
import logging
from typing import Dict, List, Optional, Tuple, Any

# è®¾ç½®PyTorchä¸»çº¿ç¨‹æ•°
torch.set_num_threads(4)
# è®¾ç½®Inter-opçº¿ç¨‹æ•°ï¼ˆé€šå¸¸1ä¸ªå°±å¤Ÿäº†ï¼‰
torch.set_num_interop_threads(1)

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("app.log", encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# é…ç½®å¸¸é‡
CONFIG = {
    "conversations_dir": "conversations",
    "models_dir": "models",
    "config_file": "app_config.json",
    "models_config_file": "models_config.json",
    "max_conversations": 100,
    "max_message_length": 8000,
    "default_timeout": 120,
    "max_input_tokens": 6000,  # å¢åŠ è¾“å…¥tokené™åˆ¶
    "reserved_tokens": 500     # ä¸ºç”Ÿæˆé¢„ç•™çš„tokenæ•°
}

# åˆ›å»ºå¿…è¦çš„ç›®å½•
Path(CONFIG["conversations_dir"]).mkdir(exist_ok=True)
Path(CONFIG["models_dir"]).mkdir(exist_ok=True)

# æ£€æŸ¥CUDAå¯ç”¨æ€§
def check_cuda_availability():
    """æ£€æŸ¥CUDAå¯ç”¨æ€§å¹¶è¿”å›è¯¦ç»†ä¿¡æ¯"""
    cuda_info = {
        'available': torch.cuda.is_available(),
        'device_count': torch.cuda.device_count() if torch.cuda.is_available() else 0,
        'current_device': torch.cuda.current_device() if torch.cuda.is_available() else None,
        'device_name': None,
        'cuda_version': torch.version.cuda if torch.cuda.is_available() else None
    }
    
    if cuda_info['available'] and cuda_info['device_count'] > 0:
        try:
            cuda_info['device_name'] = torch.cuda.get_device_name(0)
            # æ£€æŸ¥GPUå†…å­˜
            if torch.cuda.is_available():
                torch.cuda.init()
                cuda_info['memory_allocated'] = torch.cuda.memory_allocated(0) / (1024**3)
                cuda_info['memory_reserved'] = torch.cuda.memory_reserved(0) / (1024**3)
                cuda_info['total_memory'] = torch.cuda.get_device_properties(0).total_memory / (1024**3)
        except Exception as e:
            logger.error(f"è·å–CUDAè®¾å¤‡ä¿¡æ¯å¤±è´¥: {e}")
    
    return cuda_info

# æ‰“å°CUDAä¿¡æ¯
cuda_info = check_cuda_availability()
logger.info(f"CUDAå¯ç”¨æ€§: {cuda_info['available']}")
logger.info(f"GPUæ•°é‡: {cuda_info['device_count']}")
if cuda_info['available']:
    logger.info(f"GPUåç§°: {cuda_info['device_name']}")
    logger.info(f"CUDAç‰ˆæœ¬: {cuda_info['cuda_version']}")
    logger.info(f"GPUæ€»å†…å­˜: {cuda_info['total_memory']:.2f} GB")

# ä»é…ç½®æ–‡ä»¶åŠ è½½æ¨¡å‹åˆ—è¡¨
def load_models_config():
    """ä»é…ç½®æ–‡ä»¶åŠ è½½æ¨¡å‹åˆ—è¡¨"""
    default_models = {
        "Qwen3-1.7B": "/Data/llm_modl_data/qwen3-1.7B",
        "Qwen3-4B-Thinking-FP8": "/Data/llm_modl_data/qwen3-4B-Thinking-2507-FP8"
    }
    
    try:
        if os.path.exists(CONFIG["models_config_file"]):
            with open(CONFIG["models_config_file"], 'r', encoding='utf-8') as f:
                config = json.load(f)
                return config.get("models", default_models)
    except Exception as e:
        logger.error(f"åŠ è½½æ¨¡å‹é…ç½®å¤±è´¥: {e}")
    
    return default_models

def save_models_config(models):
    """ä¿å­˜æ¨¡å‹é…ç½®"""
    try:
        with open(CONFIG["models_config_file"], 'w', encoding='utf-8') as f:
            json.dump({"models": models}, f, ensure_ascii=False, indent=2)
    except Exception as e:
        logger.error(f"ä¿å­˜æ¨¡å‹é…ç½®å¤±è´¥: {e}")

# åŠ è½½æ¨¡å‹é…ç½®
MODEL_PATHS = load_models_config()

# å…¨å±€çŠ¶æ€
class GlobalState:
    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.current_model = None
        self.current_conversation_id = None
        self.conversations = {}
        self.stats = {
            'total_requests': 0,
            'total_tokens': 0,
            'total_time': 0,
            'start_time': datetime.now(),
            'failed_requests': 0
        }
        self.generation_stop_event = None
        self.is_generating = False
        self.model_lock = threading.Lock()  # æ¨¡å‹è®¿é—®é”
        self.model_max_length = 8192  # å‡è®¾æ¨¡å‹æœ€å¤§é•¿åº¦
        
        # é»˜è®¤å‚æ•°
        self.default_params = {
            'max_new_tokens': 2048,
            'temperature': 0.7,
            'top_p': 0.9,
            'repetition_penalty': 1.1,
            'max_history': 10,
            'top_k': 50,
            'do_sample': True
        }
        self.current_params = self.default_params.copy()

state = GlobalState()

# é…ç½®ç®¡ç†
def load_config():
    """åŠ è½½åº”ç”¨é…ç½®"""
    try:
        if os.path.exists(CONFIG["config_file"]):
            with open(CONFIG["config_file"], 'r', encoding='utf-8') as f:
                return json.load(f)
    except Exception as e:
        logger.error(f"åŠ è½½é…ç½®å¤±è´¥: {e}")
    return {"last_model": None, "last_conversation": None}

def save_config(config):
    """ä¿å­˜åº”ç”¨é…ç½®"""
    try:
        with open(CONFIG["config_file"], 'w', encoding='utf-8') as f:
            json.dump(config, f, ensure_ascii=False, indent=2)
    except Exception as e:
        logger.error(f"ä¿å­˜é…ç½®å¤±è´¥: {e}")

# å¯¹è¯ç®¡ç†
def load_conversations():
    """åŠ è½½æ‰€æœ‰å¯¹è¯"""
    state.conversations = {}
    conversations_dir = Path(CONFIG["conversations_dir"])
    
    for file_path in conversations_dir.glob("*.json"):
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                conversation_data = json.load(f)
                state.conversations[conversation_data["id"]] = conversation_data
        except Exception as e:
            logger.error(f"åŠ è½½å¯¹è¯å¤±è´¥ {file_path}: {e}")
    
    # æŒ‰æ›´æ–°æ—¶é—´æ’åºï¼Œåªä¿ç•™æœ€æ–°çš„Nä¸ªå¯¹è¯
    sorted_convos = sorted(
        state.conversations.values(),
        key=lambda x: x.get("updated_at", ""),
        reverse=True
    )[:CONFIG["max_conversations"]]
    
    state.conversations = {conv["id"]: conv for conv in sorted_convos}
    logger.info(f"å·²åŠ è½½ {len(state.conversations)} ä¸ªå¯¹è¯")

def save_conversation(conversation_id=None):
    """ä¿å­˜å½“å‰å¯¹è¯"""
    if conversation_id not in state.conversations:
        return
    
    conversation = state.conversations[conversation_id]
    conversation["updated_at"] = datetime.now().isoformat()
    
    file_path = Path(CONFIG["conversations_dir"]) / f"{conversation_id}.json"
    try:
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(conversation, f, ensure_ascii=False, indent=2)
    except Exception as e:
        logger.error(f"ä¿å­˜å¯¹è¯å¤±è´¥: {e}")

def create_new_conversation(title="æ–°å¯¹è¯"):
    """åˆ›å»ºæ–°å¯¹è¯"""
    conversation_id = str(uuid.uuid4())
    state.conversations[conversation_id] = {
        "id": conversation_id,
        "title": title,
        "messages": [],
        "created_at": datetime.now().isoformat(),
        "updated_at": datetime.now().isoformat(),
        "model_used": state.current_model
    }
    state.current_conversation_id = conversation_id
    save_conversation(conversation_id)
    logger.info(f"åˆ›å»ºæ–°å¯¹è¯: {title}")
    return conversation_id

def delete_conversation(conversation_id):
    """åˆ é™¤å¯¹è¯"""
    if conversation_id in state.conversations:
        title = state.conversations[conversation_id]["title"]
        del state.conversations[conversation_id]
        file_path = Path(CONFIG["conversations_dir"]) / f"{conversation_id}.json"
        if file_path.exists():
            file_path.unlink()
        
        # å¦‚æœåˆ é™¤çš„æ˜¯å½“å‰å¯¹è¯ï¼Œåˆ‡æ¢åˆ°æœ€æ–°å¯¹è¯æˆ–åˆ›å»ºæ–°å¯¹è¯
        if state.current_conversation_id == conversation_id:
            if state.conversations:
                state.current_conversation_id = list(state.conversations.keys())[0]
            else:
                create_new_conversation()
        
        logger.info(f"åˆ é™¤å¯¹è¯: {title}")
        return True
    return False

def get_conversation_history(conversation_id):
    """è·å–å¯¹è¯çš„Gradioæ ¼å¼å†å²"""
    if conversation_id not in state.conversations:
        return []
    
    messages = state.conversations[conversation_id]["messages"]
    history = []
    for i in range(0, len(messages), 2):
        if i + 1 < len(messages):
            history.append([
                messages[i]["content"],
                messages[i + 1]["content"]
            ])
    return history

# æ–‡æœ¬å¤„ç†
def clean_text(text):
    """æ¸…ç†æ–‡æœ¬ä¸­çš„éæ³•UTF-8å­—ç¬¦"""
    if not text:
        return ""
    try:
        # ç§»é™¤æ§åˆ¶å­—ç¬¦ä½†ä¿ç•™æ¢è¡Œå’Œåˆ¶è¡¨ç¬¦
        cleaned = re.sub(r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F]', '', text)
        return cleaned
    except:
        return re.sub(r'[^\x00-\x7F\u4e00-\u9fff]+', '', text)

def truncate_text(text, max_length=CONFIG["max_message_length"]):
    """æˆªæ–­æ–‡æœ¬åˆ°æŒ‡å®šé•¿åº¦"""
    if len(text) > max_length:
        return text[:max_length] + "...(å·²æˆªæ–­)"
    return text

# ç³»ç»Ÿç›‘æ§
def get_system_info():
    """è·å–ç³»ç»Ÿä¿¡æ¯"""
    info = {}
    
    try:
        # CPUä¿¡æ¯
        info['cpu_usage'] = psutil.cpu_percent(interval=0.1)
        info['memory_usage'] = psutil.virtual_memory().percent
        info['memory_used_gb'] = psutil.virtual_memory().used / (1024**3)
        info['memory_total_gb'] = psutil.virtual_memory().total / (1024**3)
        
        # CUDAä¿¡æ¯
        info['cuda_available'] = torch.cuda.is_available()
        info['cuda_device_count'] = torch.cuda.device_count() if torch.cuda.is_available() else 0
        
        if info['cuda_available'] and info['cuda_device_count'] > 0:
            try:
                # PyTorch GPUä¿¡æ¯
                info['torch_gpu_name'] = torch.cuda.get_device_name(0)
                info['torch_gpu_memory_allocated'] = torch.cuda.memory_allocated(0) / (1024**3)
                info['torch_gpu_memory_reserved'] = torch.cuda.memory_reserved(0) / (1024**3)
                info['torch_gpu_total_memory'] = torch.cuda.get_device_properties(0).total_memory / (1024**3)
                
                # GPUtilä¿¡æ¯ï¼ˆå¤‡ç”¨ï¼‰
                gpus = GPUtil.getGPUs()
                if gpus:
                    gpu = gpus[0]
                    info['gpu_name'] = gpu.name
                    info['gpu_usage'] = gpu.load * 100
                    info['gpu_memory_used'] = gpu.memoryUsed
                    info['gpu_memory_total'] = gpu.memoryTotal
                    info['gpu_temperature'] = gpu.temperature
                else:
                    # å¦‚æœæ²¡æœ‰GPUtilä¿¡æ¯ï¼Œä½¿ç”¨PyTorchçš„ä¿¡æ¯
                    info['gpu_name'] = info['torch_gpu_name']
                    info['gpu_usage'] = (info['torch_gpu_memory_allocated'] / info['torch_gpu_total_memory']) * 100
                    info['gpu_memory_used'] = info['torch_gpu_memory_allocated'] * 1024  # è½¬æ¢ä¸ºMB
                    info['gpu_memory_total'] = info['torch_gpu_total_memory'] * 1024     # è½¬æ¢ä¸ºMB
                    info['gpu_temperature'] = 0  # PyTorchä¸æä¾›æ¸©åº¦ä¿¡æ¯
            except Exception as e:
                logger.warning(f"è·å–GPUä¿¡æ¯å¤±è´¥: {e}")
                info['cuda_available'] = False
        
        # ç£ç›˜ä¿¡æ¯
        disk_usage = psutil.disk_usage('/')
        info['disk_usage'] = disk_usage.percent
        info['disk_free_gb'] = disk_usage.free / (1024**3)
        
    except Exception as e:
        logger.error(f"è·å–ç³»ç»Ÿä¿¡æ¯å¤±è´¥: {e}")
    
    return info

def get_system_info_html():
    """è·å–ç³»ç»Ÿä¿¡æ¯çš„HTMLæ˜¾ç¤º"""
    system_info = get_system_info()
    
    html = """
    <div style="background: linear-gradient(135deg, #4facfe 0%, #00f2fe 100%); color: white; padding: 15px; border-radius: 10px; margin: 10px 0;">
        <h3 style="margin-top: 0; margin-bottom: 15px;">ğŸ’» ç³»ç»Ÿç›‘æ§</h3>
    """
    
    try:
        # CPUå’Œå†…å­˜
        html += f"""
            <div style="margin: 10px 0;">
                <div style="display: flex; justify-content: space-between; font-size: 0.9em;">
                    <span>ğŸ–¥ï¸ CPU</span>
                    <span>{system_info.get('cpu_usage', 0):.1f}%</span>
                </div>
                <div style="background: rgba(255,255,255,0.3); border-radius: 10px; height: 6px; margin-top: 5px;">
                    <div style="background: #ff6b6b; width: {system_info.get('cpu_usage', 0)}%; height: 100%; border-radius: 10px;"></div>
                </div>
            </div>
            
            <div style="margin: 10px 0;">
                <div style="display: flex; justify-content: space-between; font-size: 0.9em;">
                    <span>ğŸ’¾ å†…å­˜</span>
                    <span>{system_info.get('memory_usage', 0):.1f}%</span>
                </div>
                <div style="background: rgba(255,255,255,0.3); border-radius: 10px; height: 6px; margin-top: 5px;">
                    <div style="background: #4ecdc4; width: {system_info.get('memory_usage', 0)}%; height: 100%; border-radius: 10px;"></div>
                </div>
            </div>
        """
        
        # GPUä¿¡æ¯
        if system_info.get('cuda_available', False):
            gpu_memory_usage = (system_info.get('torch_gpu_memory_allocated', 0) / system_info.get('torch_gpu_total_memory', 1)) * 100
            
            html += f"""
                <div style="margin: 10px 0;">
                    <div style="display: flex; justify-content: space-between; font-size: 0.9em;">
                        <span>ğŸ® GPUå†…å­˜</span>
                        <span>{gpu_memory_usage:.1f}%</span>
                    </div>
                    <div style="background: rgba(255,255,255,0.3); border-radius: 10px; height: 6px; margin-top: 5px;">
                        <div style="background: #45b7d1; width: {gpu_memory_usage}%; height: 100%; border-radius: 10px;"></div>
                    </div>
                </div>
                
                <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 10px; margin-top: 10px;">
                    <div style="text-align: center;">
                        <div style="font-size: 0.9em; font-weight: bold;">{system_info.get('cuda_device_count', 0)}</div>
                        <div style="font-size: 0.7em;">GPUæ•°é‡</div>
                    </div>
                    <div style="text-align: center;">
                        <div style="font-size: 0.9em; font-weight: bold;">{system_info.get('torch_gpu_memory_allocated', 0):.1f}G</div>
                        <div style="font-size: 0.7em;">å·²ç”¨æ˜¾å­˜</div>
                    </div>
                </div>
                <div style="margin-top: 5px; font-size: 0.7em; text-align: center;">
                    {system_info.get('torch_gpu_name', 'æœªçŸ¥GPU')}
                </div>
            """
        else:
            html += """
                <div style="margin: 10px 0; text-align: center; color: #ff6b6b;">
                    âš ï¸ CUDAä¸å¯ç”¨ï¼Œæ¨¡å‹è¿è¡Œåœ¨CPUä¸Š
                </div>
            """
        
        # ç£ç›˜ä¿¡æ¯
        html += f"""
            <div style="margin: 10px 0;">
                <div style="display: flex; justify-content: space-between; font-size: 0.9em;">
                    <span>ğŸ’¾ ç£ç›˜</span>
                    <span>{system_info.get('disk_usage', 0):.1f}%</span>
                </div>
                <div style="background: rgba(255,255,255,0.3); border-radius: 10px; height: 6px; margin-top: 5px;">
                    <div style="background: #f9c74f; width: {system_info.get('disk_usage', 0)}%; height: 100%; border-radius: 10px;"></div>
                </div>
            </div>
        """
    
    except Exception as e:
        logger.error(f"ç”Ÿæˆç³»ç»Ÿä¿¡æ¯HTMLå¤±è´¥: {e}")
        html += "<div style='color: #ff6b6b;'>ç³»ç»Ÿä¿¡æ¯è·å–å¤±è´¥</div>"
    
    html += "</div>"
    return html

def get_stats_html():
    """è·å–ç»Ÿè®¡ä¿¡æ¯çš„HTMLæ˜¾ç¤º"""
    try:
        run_time = datetime.now() - state.stats['start_time']
        hours = run_time.total_seconds() // 3600
        minutes = (run_time.total_seconds() % 3600) // 60
        
        avg_time = state.stats['total_time'] / max(state.stats['total_requests'], 1)
        current_conv = state.conversations.get(state.current_conversation_id, {})
        
        success_rate = 100
        if state.stats['total_requests'] > 0:
            success_rate = ((state.stats['total_requests'] - state.stats['failed_requests']) / state.stats['total_requests']) * 100
        
        # è·å–å½“å‰æ¨¡å‹å’Œè®¾å¤‡ä¿¡æ¯
        device_info = "CPU"
        if state.model is not None:
            device_info = str(state.model.device)
            if 'cuda' in device_info:
                device_info = f"GPU:{device_info.split(':')[-1]}"
        
        html = f"""
        <div style="background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%); color: white; padding: 15px; border-radius: 10px; margin: 10px 0;">
            <h3 style="margin-top: 0; margin-bottom: 15px;">ğŸ“Š ä½¿ç”¨ç»Ÿè®¡</h3>
            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 10px;">
                <div style="text-align: center;">
                    <div style="font-size: 1.3em; font-weight: bold;">{state.stats['total_requests']}</div>
                    <div style="font-size: 0.8em;">è¯·æ±‚æ•°</div>
                </div>
                <div style="text-align: center;">
                    <div style="font-size: 1.3em; font-weight: bold;">{state.stats['total_tokens']}</div>
                    <div style="font-size: 0.8em;">Tokenæ•°</div>
                </div>
                <div style="text-align: center;">
                    <div style="font-size: 1.3em; font-weight: bold;">{avg_time:.1f}s</div>
                    <div style="font-size: 0.8em;">å¹³å‡å“åº”</div>
                </div>
                <div style="text-align: center;">
                    <div style="font-size: 1.3em; font-weight: bold;">{success_rate:.1f}%</div>
                    <div style="font-size: 0.8em;">æˆåŠŸç‡</div>
                </div>
            </div>
            <div style="text-align: center; margin-top: 10px; font-size: 0.8em;">
                è¿è¡Œ: {int(hours)}h{int(minutes)}m | å¯¹è¯: {len(state.conversations)}<br>
                è®¾å¤‡: {device_info} | æ¨¡å‹: {state.current_model or 'æ— '}
            </div>
        </div>
        """
        return html
    except Exception as e:
        logger.error(f"ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯HTMLå¤±è´¥: {e}")
        return "<div>ç»Ÿè®¡ä¿¡æ¯åŠ è½½å¤±è´¥</div>"

# æ¨¡å‹ç®¡ç†
def get_model_info(model_path):
    """è·å–æ¨¡å‹åŸºæœ¬ä¿¡æ¯"""
    try:
        model_name = os.path.basename(model_path)
        
        info = {
            "name": model_name,
            "path": model_path,
            "parameters": "æœªçŸ¥",
            "type": "æœªçŸ¥",
            "size_gb": "æœªçŸ¥"
        }
        
        # ä¼°ç®—å‚æ•°è§„æ¨¡
        if "1.7B" in model_name or "1.7b" in model_name:
            info["parameters"] = "17äº¿"
            info["size_gb"] = "~3.5GB"
        elif "4B" in model_name or "4b" in model_name:
            info["parameters"] = "40äº¿"
            info["size_gb"] = "~8GB"
        elif "7B" in model_name or "7b" in model_name:
            info["parameters"] = "70äº¿"
            info["size_gb"] = "~14GB"
        elif "14B" in model_name or "14b" in model_name:
            info["parameters"] = "140äº¿"
            info["size_gb"] = "~28GB"
            
        # åˆ¤æ–­æ¨¡å‹ç±»å‹
        if "Thinking" in model_name:
            info["type"] = "æ€è€ƒå¢å¼ºå‹"
        elif "Chat" in model_name:
            info["type"] = "å¯¹è¯ä¼˜åŒ–å‹"
        elif "Instruct" in model_name:
            info["type"] = "æŒ‡ä»¤è°ƒä¼˜å‹"
        else:
            info["type"] = "åŸºç¡€æ¨¡å‹"
            
        return info
    except Exception as e:
        logger.error(f"è·å–æ¨¡å‹ä¿¡æ¯å¤±è´¥: {e}")
        return {
            "name": os.path.basename(model_path),
            "path": model_path,
            "parameters": "æœªçŸ¥",
            "type": "æœªçŸ¥",
            "size_gb": "æœªçŸ¥"
        }

def load_model_with_fallback(model_path, model_display_name):
    """åŠ è½½æ¨¡å‹ï¼Œå¸¦æœ‰GPUå›é€€æœºåˆ¶"""
    cuda_info = check_cuda_availability()
    
    if not cuda_info['available']:
        logger.warning("CUDAä¸å¯ç”¨ï¼Œå°†åŠ è½½æ¨¡å‹åˆ°CPU")
        return load_model_to_cpu(model_path, model_display_name)
    
    # æ£€æŸ¥GPUå†…å­˜æ˜¯å¦è¶³å¤Ÿ
    available_memory_gb = cuda_info['total_memory'] - cuda_info['memory_allocated']
    
    # å°è¯•GPUåŠ è½½
    try:
        logger.info(f"å°è¯•åŠ è½½æ¨¡å‹åˆ°GPU: {model_display_name}")
        
        # æ¸…é™¤GPUç¼“å­˜
        torch.cuda.empty_cache()
        gc.collect()
        
        # åŠ è½½åˆ†è¯å™¨
        state.tokenizer = AutoTokenizer.from_pretrained(
            model_path,
            trust_remote_code=True
        )
        
        # æ–¹æ³•1: ä½¿ç”¨device_map="auto"ï¼ˆæ¨èç”¨äºå¤šGPUï¼‰
        try:
            logger.info("å°è¯•æ–¹æ³•1: device_map='auto'")
            state.model = AutoModelForCausalLM.from_pretrained(
                model_path,
                torch_dtype=torch.float16,
                device_map="auto",
                low_cpu_mem_usage=True,
                trust_remote_code=True
            )
        except Exception as e1:
            logger.warning(f"æ–¹æ³•1å¤±è´¥: {e1}")
            
            # æ–¹æ³•2: ä½¿ç”¨device_map="cuda"
            try:
                logger.info("å°è¯•æ–¹æ³•2: device_map='cuda'")
                state.model = AutoModelForCausalLM.from_pretrained(
                    model_path,
                    torch_dtype=torch.float16,
                    device_map="cuda",
                    low_cpu_mem_usage=True,
                    trust_remote_code=True
                )
            except Exception as e2:
                logger.warning(f"æ–¹æ³•2å¤±è´¥: {e2}")
                
                # æ–¹æ³•3: æ‰‹åŠ¨æŒ‡å®šè®¾å¤‡
                try:
                    logger.info("å°è¯•æ–¹æ³•3: æ‰‹åŠ¨æŒ‡å®šè®¾å¤‡")
                    state.model = AutoModelForCausalLM.from_pretrained(
                        model_path,
                        torch_dtype=torch.float16,
                        device_map=None,
                        low_cpu_mem_usage=True,
                        trust_remote_code=True
                    )
                    state.model = state.model.to('cuda')
                except Exception as e3:
                    logger.error(f"æ‰€æœ‰GPUåŠ è½½æ–¹æ³•éƒ½å¤±è´¥: {e3}")
                    raise
        
        state.current_model = model_display_name
        
        # éªŒè¯æ¨¡å‹æ˜¯å¦åœ¨GPUä¸Š
        if hasattr(state.model, 'device'):
            device_str = str(state.model.device)
            logger.info(f"æ¨¡å‹åŠ è½½æˆåŠŸï¼Œè®¾å¤‡: {device_str}")
            
            if 'cuda' not in device_str:
                logger.warning("æ¨¡å‹æœªåœ¨GPUä¸Šè¿è¡Œï¼Œå°†å›é€€åˆ°CPU")
                return load_model_to_cpu(model_path, model_display_name)
        else:
            # æ£€æŸ¥ç¬¬ä¸€ä¸ªå‚æ•°çš„è®¾å¤‡
            for param in state.model.parameters():
                device_str = str(param.device)
                logger.info(f"æ¨¡å‹å‚æ•°è®¾å¤‡: {device_str}")
                break
        
        # ä¿å­˜é…ç½®
        config = load_config()
        config["last_model"] = model_display_name
        save_config(config)
        
        system_info = get_system_info()
        device_info = f"è¿è¡Œè®¾å¤‡: {state.model.device}"
        
        if system_info.get('cuda_available', False):
            device_info += f" | GPUå†…å­˜å ç”¨: {system_info.get('torch_gpu_memory_allocated', 0):.2f} GB"
        
        logger.info(f"æ¨¡å‹GPUåŠ è½½æˆåŠŸ: {model_display_name}")
        return f"âœ… **{model_display_name}** åŠ è½½æˆåŠŸï¼\n\n{device_info}", get_model_display_info(model_path)
        
    except Exception as e:
        logger.error(f"GPUåŠ è½½å¤±è´¥: {e}")
        # å›é€€åˆ°CPU
        return load_model_to_cpu(model_path, model_display_name)

def load_model_to_cpu(model_path, model_display_name):
    """åŠ è½½æ¨¡å‹åˆ°CPU"""
    try:
        logger.info(f"åŠ è½½æ¨¡å‹åˆ°CPU: {model_display_name}")
        
        # åŠ è½½åˆ†è¯å™¨
        state.tokenizer = AutoTokenizer.from_pretrained(
            model_path,
            trust_remote_code=True
        )
        
        # åŠ è½½æ¨¡å‹åˆ°CPU
        state.model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.float32,  # CPUä¸Šä½¿ç”¨float32
            device_map=None,
            low_cpu_mem_usage=True,
            trust_remote_code=True
        )
        state.model = state.model.to('cpu')
        state.current_model = model_display_name
        
        # ä¿å­˜é…ç½®
        config = load_config()
        config["last_model"] = model_display_name
        save_config(config)
        
        logger.info(f"æ¨¡å‹CPUåŠ è½½æˆåŠŸ: {model_display_name}")
        return f"âœ… **{model_display_name}** å·²åŠ è½½åˆ°CPU", get_model_display_info(model_path)
    except Exception as e:
        logger.error(f"CPUåŠ è½½å¤±è´¥: {e}")
        return f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}", ""

def load_model(model_path, model_display_name):
    """åŠ è½½æ¨¡å‹ï¼ˆä¸»å‡½æ•°ï¼‰"""
    with state.model_lock:
        try:
            logger.info(f"å¼€å§‹åŠ è½½æ¨¡å‹: {model_display_name}")
            
            # å¦‚æœå·²ç»æœ‰æ¨¡å‹åŠ è½½ï¼Œå…ˆå¸è½½
            if state.model is not None:
                unload_model()
            
            # æ£€æŸ¥æ¨¡å‹è·¯å¾„æ˜¯å¦å­˜åœ¨
            if not os.path.exists(model_path):
                return f"âŒ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}", ""
            
            # å°è¯•åŠ è½½æ¨¡å‹ï¼Œå¸¦æœ‰GPUå›é€€æœºåˆ¶
            return load_model_with_fallback(model_path, model_display_name)
            
        except Exception as e:
            logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}", ""

def unload_model():
    """å¸è½½æ¨¡å‹ - å¢å¼ºç‰ˆæœ¬"""
    with state.model_lock:
        if state.model is not None:
            model_name = state.current_model
            
            # è®°å½•å¸è½½å‰çš„å†…å­˜ä½¿ç”¨
            if torch.cuda.is_available():
                allocated_before = torch.cuda.memory_allocated() / (1024**3)
                reserved_before = torch.cuda.memory_reserved() / (1024**3)
                logger.info(f"å¸è½½å‰ - å·²åˆ†é…: {allocated_before:.2f}GB, ä¿ç•™: {reserved_before:.2f}GB")
            
            try:
                # é¦–å…ˆå°†æ¨¡å‹ç§»å›CPUï¼ˆå¦‚æœå®ƒåœ¨GPUä¸Šï¼‰
                if hasattr(state.model, 'device') and 'cuda' in str(state.model.device):
                    try:
                        state.model = state.model.cpu()
                        logger.info("æ¨¡å‹å·²ç§»åŠ¨åˆ°CPU")
                        # ç»™GPUä¸€äº›æ—¶é—´æ¥é‡Šæ”¾å†…å­˜
                        time.sleep(0.5)
                    except Exception as e:
                        logger.warning(f"ç§»åŠ¨æ¨¡å‹åˆ°CPUå¤±è´¥: {e}")
                
                # åˆ é™¤æ¨¡å‹å’Œåˆ†è¯å™¨
                del state.model
                del state.tokenizer
                
            except Exception as e:
                logger.warning(f"åˆ é™¤æ¨¡å‹å¼•ç”¨æ—¶å‡ºé”™: {e}")
            finally:
                state.model = None
                state.tokenizer = None
                state.current_model = None
            
            # å¼ºåˆ¶åƒåœ¾å›æ”¶
            for i in range(3):  # å¤šæ¬¡åƒåœ¾å›æ”¶
                gc.collect()
                time.sleep(0.1)
            
            # æ¸…ç†GPUå†…å­˜
            if torch.cuda.is_available():
                try:
                    # æ¸…ç©ºç¼“å­˜
                    torch.cuda.empty_cache()
                    
                    # åŒæ­¥æ‰€æœ‰è®¾å¤‡
                    for i in range(torch.cuda.device_count()):
                        torch.cuda.synchronize(i)
                    
                    # å†æ¬¡æ¸…ç©ºç¼“å­˜
                    torch.cuda.empty_cache()
                    
                    # è®°å½•å¸è½½åçš„å†…å­˜ä½¿ç”¨
                    allocated_after = torch.cuda.memory_allocated() / (1024**3)
                    reserved_after = torch.cuda.memory_reserved() / (1024**3)
                    
                    memory_freed = allocated_before - allocated_after
                    logger.info(f"å¸è½½å - å·²åˆ†é…: {allocated_after:.2f}GB, ä¿ç•™: {reserved_after:.2f}GB")
                    logger.info(f"é‡Šæ”¾å†…å­˜: {memory_freed:.2f}GB")
                    
                    # å¦‚æœé‡Šæ”¾çš„å†…å­˜å¾ˆå°‘ï¼Œè®°å½•è­¦å‘Š
                    if memory_freed < 0.1:
                        logger.warning("æ¨¡å‹å¸è½½åé‡Šæ”¾çš„å†…å­˜å¾ˆå°‘ï¼Œå¯èƒ½ä»æœ‰å¼•ç”¨æœªæ¸…é™¤")
                        
                except Exception as e:
                    logger.warning(f"é‡ç½®GPUå†…å­˜å¤±è´¥: {e}")
            
            logger.info(f"æ¨¡å‹å·²å¸è½½: {model_name}")
            return "âœ… æ¨¡å‹å·²å¸è½½", ""
        return "â„¹ï¸ æ²¡æœ‰åŠ è½½çš„æ¨¡å‹", ""

def get_model_display_info(model_path):
    """è·å–æ¨¡å‹æ˜¾ç¤ºä¿¡æ¯"""
    info = get_model_info(model_path)
    
    # è·å–å½“å‰è®¾å¤‡ä¿¡æ¯
    device_info = "CPU"
    if state.model is not None:
        device_info = str(state.model.device)
    
    cuda_info = check_cuda_availability()
    gpu_status = "âœ… å¯ç”¨" if cuda_info['available'] else "âŒ ä¸å¯ç”¨"
    
    return f"""
    <div style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 15px; border-radius: 10px; margin: 10px 0;">
        <h3 style="margin-top: 0; margin-bottom: 10px;">ğŸ“ æ¨¡å‹ä¿¡æ¯</h3>
        <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 10px;">
            <div style="text-align: center;">
                <div style="font-size: 1.2em; font-weight: bold;">{info['parameters']}</div>
                <div style="font-size: 0.8em;">å‚æ•°è§„æ¨¡</div>
            </div>
            <div style="text-align: center;">
                <div style="font-size: 1.2em; font-weight: bold;">{info['type']}</div>
                <div style="font-size: 0.8em;">æ¨¡å‹ç±»å‹</div>
            </div>
            <div style="text-align: center;">
                <div style="font-size: 1.2em; font-weight: bold;">{info['size_gb']}</div>
                <div style="font-size: 0.8em;">é¢„ä¼°å¤§å°</div>
            </div>
            <div style="text-align: center;">
                <div style="font-size: 1.2em; font-weight: bold;">{device_info}</div>
                <div style="font-size: 0.8em;">è¿è¡Œè®¾å¤‡</div>
            </div>
        </div>
        <div style="margin-top: 10px; font-size: 0.8em; text-align: center;">
            GPUçŠ¶æ€: {gpu_status} | è·¯å¾„: {os.path.basename(model_path)}
        </div>
    </div>
    """

# æ™ºèƒ½å†å²æ¶ˆæ¯æˆªæ–­
def smart_truncate_messages(messages, max_tokens=CONFIG["max_input_tokens"]):
    """æ™ºèƒ½æˆªæ–­å†å²æ¶ˆæ¯ï¼Œä¿ç•™æœ€é‡è¦çš„ä¸Šä¸‹æ–‡"""
    if not state.tokenizer:
        return messages
    
    # è®¡ç®—å½“å‰æ¶ˆæ¯çš„tokenæ•°é‡
    def count_tokens(msg_list):
        total_tokens = 0
        for msg in msg_list:
            try:
                tokens = state.tokenizer.encode(msg["content"], add_special_tokens=False)
                total_tokens += len(tokens)
            except:
                # å¦‚æœç¼–ç å¤±è´¥ï¼Œä½¿ç”¨å­—ç¬¦æ•°ä¼°ç®—
                total_tokens += len(msg["content"]) // 4
        return total_tokens
    
    current_tokens = count_tokens(messages)
    
    # å¦‚æœtokenæ•°åœ¨é™åˆ¶å†…ï¼Œç›´æ¥è¿”å›
    if current_tokens <= max_tokens:
        return messages
    
    logger.info(f"è¾“å…¥tokenæ•°({current_tokens})è¶…è¿‡é™åˆ¶({max_tokens})ï¼Œè¿›è¡Œæ™ºèƒ½æˆªæ–­")
    
    # æ™ºèƒ½æˆªæ–­ç­–ç•¥ï¼š
    # 1. å§‹ç»ˆä¿ç•™ç³»ç»Ÿæ¶ˆæ¯ï¼ˆå¦‚æœæœ‰ï¼‰
    # 2. ä¼˜å…ˆä¿ç•™æœ€è¿‘çš„å¯¹è¯
    # 3. å¦‚æœä»ç„¶è¶…é•¿ï¼Œé€æ­¥åˆ é™¤æœ€æ—§çš„æ¶ˆæ¯
    
    truncated_messages = messages.copy()
    
    # é¦–å…ˆå°è¯•åˆ é™¤æœ€æ—§çš„æ¶ˆæ¯ï¼ˆé™¤äº†ç³»ç»Ÿæ¶ˆæ¯ï¼‰
    while count_tokens(truncated_messages) > max_tokens and len(truncated_messages) > 1:
        # è·³è¿‡ç³»ç»Ÿæ¶ˆæ¯ï¼ˆå¦‚æœæœ‰ï¼‰
        if truncated_messages[0].get("role") == "system":
            if len(truncated_messages) > 2:
                # åˆ é™¤ç¬¬ä¸€ä¸ªéç³»ç»Ÿæ¶ˆæ¯
                truncated_messages.pop(1)
            else:
                break
        else:
            # åˆ é™¤ç¬¬ä¸€æ¡æ¶ˆæ¯
            truncated_messages.pop(0)
    
    # å¦‚æœä»ç„¶è¶…é•¿ï¼Œå°è¯•æˆªæ–­å•æ¡æ¶ˆæ¯çš„å†…å®¹
    if count_tokens(truncated_messages) > max_tokens:
        for i, msg in enumerate(truncated_messages):
            if len(msg["content"]) > 500:  # åªæˆªæ–­é•¿æ¶ˆæ¯
                truncated_messages[i]["content"] = msg["content"][:500] + "...(å·²æˆªæ–­)"
                if count_tokens(truncated_messages) <= max_tokens:
                    break
    
    final_tokens = count_tokens(truncated_messages)
    logger.info(f"æˆªæ–­åtokenæ•°: {final_tokens}, ä¿ç•™äº† {len(truncated_messages)} æ¡æ¶ˆæ¯")
    
    return truncated_messages

# å¯¹è¯ç”Ÿæˆ - ä¿®å¤ç‰ˆæœ¬
def chat_stream(message, conversation_id):
    """æµå¼èŠå¤©å‡½æ•° - ä¿®å¤è¾“å…¥æˆªæ–­é—®é¢˜"""
    if state.model is None or state.tokenizer is None:
        yield get_conversation_history(conversation_id) + [[message, "âš ï¸ è¯·å…ˆåŠ è½½æ¨¡å‹ï¼"]], get_stats_html(), get_system_info_html()
        return
    
    # è®¾ç½®ç”ŸæˆçŠ¶æ€
    state.is_generating = True
    state.generation_stop_event = threading.Event()
    
    try:
        # è·å–å¯¹è¯å†å²
        conversation = state.conversations[conversation_id]
        
        # ä½¿ç”¨max_historyå‚æ•°é™åˆ¶å†å²æ¶ˆæ¯æ•°é‡
        max_history = state.current_params['max_history']
        all_messages = conversation["messages"]
        
        # è®¡ç®—è¦ä¿ç•™çš„æ¶ˆæ¯æ•°é‡ï¼ˆæœ€è¿‘çš„nè½®å¯¹è¯ï¼‰
        messages_to_keep = min(len(all_messages), max_history * 2)
        recent_messages = all_messages[-messages_to_keep:] if messages_to_keep > 0 else []
        
        # æ·»åŠ å½“å‰ç”¨æˆ·è¾“å…¥
        clean_message = truncate_text(clean_text(message))
        messages_for_model = recent_messages + [{"role": "user", "content": clean_message}]
        
        # æ™ºèƒ½æˆªæ–­æ¶ˆæ¯
        messages_for_model = smart_truncate_messages(messages_for_model)
        
        # åº”ç”¨èŠå¤©æ¨¡æ¿
        try:
            text = state.tokenizer.apply_chat_template(
                messages_for_model,
                tokenize=False,
                add_generation_prompt=True,
                enable_thinking=getattr(state.tokenizer, 'enable_thinking', False)
            )
        except Exception as e:
            logger.warning(f"åº”ç”¨èŠå¤©æ¨¡æ¿å¤±è´¥: {e}")
            # å›é€€åˆ°ç®€å•æ ¼å¼
            text = "\n".join([f"{msg['role']}: {msg['content']}" for msg in messages_for_model]) + "\nassistant:"
        
        # æ£€æŸ¥è¾“å…¥é•¿åº¦ï¼ˆä½¿ç”¨æ›´å®½æ¾çš„é™åˆ¶ï¼‰
        input_ids = state.tokenizer.encode(text)
        max_input_length = CONFIG["max_input_tokens"] - CONFIG["reserved_tokens"]
        
        if len(input_ids) > max_input_length:
            logger.info(f"è¾“å…¥è¿‡é•¿({len(input_ids)} tokens)ï¼Œè¿›è¡Œæˆªæ–­")
            # ä¿ç•™æ›´å¤šçš„ä¸Šä¸‹æ–‡ï¼Œä»åé¢æˆªæ–­å¯èƒ½ä¼šç ´åæ ¼å¼ï¼Œæ‰€ä»¥ä»å‰é¢åˆ é™¤ä¸€äº›å†å²
            # ä½†ä¸ºäº†ä¿æŒå®Œæ•´æ€§ï¼Œæˆ‘ä»¬ä½¿ç”¨tokenizerçš„æˆªæ–­åŠŸèƒ½
            text = state.tokenizer.decode(input_ids[:max_input_length])
            # æ·»åŠ æˆªæ–­æ ‡è®°
            if not text.endswith("..."):
                text += "...(ä¸Šä¸‹æ–‡å·²æˆªæ–­)"
        
        # å°†è¾“å…¥ç§»åŠ¨åˆ°æ¨¡å‹æ‰€åœ¨çš„è®¾å¤‡
        model_device = state.model.device if hasattr(state.model, 'device') else 'cpu'
        model_inputs = state.tokenizer([text], return_tensors="pt").to(model_device)

        # åˆ›å»ºæµå¼ç”Ÿæˆå™¨
        streamer = TextIteratorStreamer(state.tokenizer, skip_prompt=True, timeout=CONFIG["default_timeout"])
        
        # ç”Ÿæˆå‚æ•°
        generation_kwargs = dict(
            **model_inputs,
            max_new_tokens=state.current_params['max_new_tokens'],
            temperature=state.current_params['temperature'],
            top_p=state.current_params['top_p'],
            top_k=state.current_params.get('top_k', 50),
            do_sample=state.current_params.get('do_sample', True),
            pad_token_id=state.tokenizer.eos_token_id,
            repetition_penalty=state.current_params['repetition_penalty'],
            streamer=streamer
        )

        # åœ¨å•ç‹¬çº¿ç¨‹ä¸­ç”Ÿæˆ
        start_time = time.time()
        thread = threading.Thread(target=state.model.generate, kwargs=generation_kwargs)
        thread.daemon = True  # è®¾ç½®ä¸ºå®ˆæŠ¤çº¿ç¨‹
        thread.start()
        
        # æµå¼è¾“å‡º
        generated_text = ""
        thinking_content = ""
        is_thinking = True
        
        for new_text in streamer:
            # æ£€æŸ¥åœæ­¢ä¿¡å·
            if state.generation_stop_event and state.generation_stop_event.is_set():
                logger.info("ç”Ÿæˆè¢«ç”¨æˆ·åœæ­¢")
                break
                
            generated_text += new_text
            
            # å°è¯•è§£ææ€è€ƒå†…å®¹
            if is_thinking and "</think>" in generated_text:
                parts = generated_text.split("</think>", 1)
                thinking_content = parts[0] + "</think>"
                generated_text = parts[1] if len(parts) > 1 else ""
                is_thinking = False
            
            # ç»„åˆè¾“å‡º
            if is_thinking:
                partial_response = f"ğŸ¤” **æ€è€ƒä¸­...**\n\n{thinking_content + generated_text}"
            else:
                if thinking_content:
                    partial_response = f"<details style='margin-bottom: 10px;'><summary>ğŸ§  æ€è€ƒè¿‡ç¨‹</summary>\n\n{thinking_content}\n\n</details>\n\n{generated_text}"
                else:
                    partial_response = generated_text
            
            # æ¸…ç†æ–‡æœ¬
            partial_response = clean_text(partial_response)
            
            # æ›´æ–°å¯¹è¯æ˜¾ç¤º
            current_history = get_conversation_history(conversation_id) + [[clean_message, partial_response]]
            
            yield current_history, get_stats_html(), get_system_info_html()
        
        generation_time = time.time() - start_time
        
        # å¦‚æœè¢«åœæ­¢ï¼Œæ·»åŠ åœæ­¢æ ‡è®°
        if state.generation_stop_event and state.generation_stop_event.is_set():
            generated_text += "\n\n---\n*ç”Ÿæˆå·²è¢«ç”¨æˆ·åœæ­¢*"
        
        # è®¡ç®—ç”Ÿæˆçš„tokenæ•°é‡
        output_ids = state.tokenizer.encode(generated_text)
        tokens_generated = len(output_ids)
        
        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        state.stats['total_requests'] += 1
        state.stats['total_tokens'] += tokens_generated
        state.stats['total_time'] += generation_time
        
        # ä¿å­˜åˆ°å¯¹è¯å†å²
        conversation["messages"].append({"role": "user", "content": clean_message})
        conversation["messages"].append({"role": "assistant", "content": generated_text})
        conversation["model_used"] = state.current_model
        
        # æ›´æ–°å¯¹è¯æ ‡é¢˜ï¼ˆå¦‚æœæ˜¯ç¬¬ä¸€æ¡æ¶ˆæ¯ï¼‰
        if len(conversation["messages"]) == 2:
            conversation["title"] = clean_message[:20] + "..." if len(clean_message) > 20 else clean_message
        
        save_conversation(conversation_id)
        
        # æœ€ç»ˆè¾“å‡º
        if thinking_content:
            final_response = f"<details style='margin-bottom: 10px;'><summary>ğŸ§  æ€è€ƒè¿‡ç¨‹</summary>\n\n{thinking_content}\n\n</details>\n\n{generated_text}"
        else:
            final_response = generated_text
        
        # æ·»åŠ å“åº”æ—¶é—´ä¿¡æ¯
        if not (state.generation_stop_event and state.generation_stop_event.is_set()):
            final_response += f"\n\n---\n*å“åº”æ—¶é—´: {generation_time:.2f}s | ç”ŸæˆToken: {tokens_generated}*"
        
        # æœ€ç»ˆæ›´æ–°
        final_history = get_conversation_history(conversation_id)
        
        yield final_history, get_stats_html(), get_system_info_html()
        
    except Exception as e:
        logger.error(f"ç”Ÿæˆå“åº”æ—¶å‡ºé”™: {e}")
        state.stats['failed_requests'] += 1
        error_msg = f"âŒ ç”Ÿæˆå“åº”æ—¶å‡ºé”™: {str(e)}"
        yield get_conversation_history(conversation_id) + [[message, error_msg]], get_stats_html(), get_system_info_html()
    finally:
        state.is_generating = False
        state.generation_stop_event = None

def stop_generation():
    """åœæ­¢ç”Ÿæˆ"""
    if state.generation_stop_event and state.is_generating:
        state.generation_stop_event.set()
        return "ğŸ›‘ æ­£åœ¨åœæ­¢ç”Ÿæˆ..."
    return "â„¹ï¸ æ²¡æœ‰æ­£åœ¨è¿›è¡Œçš„ç”Ÿæˆ"

# å‚æ•°ç®¡ç†
def update_params(max_new_tokens, temperature, top_p, repetition_penalty, max_history, top_k, do_sample):
    """æ›´æ–°å‚æ•°"""
    state.current_params.update({
        'max_new_tokens': max_new_tokens,
        'temperature': temperature,
        'top_p': top_p,
        'repetition_penalty': repetition_penalty,
        'max_history': max_history,
        'top_k': top_k,
        'do_sample': do_sample
    })
    return f"âœ… å‚æ•°å·²æ›´æ–° | é•¿åº¦: {max_new_tokens} | æ¸©åº¦: {temperature}"

def reset_params():
    """é‡ç½®å‚æ•°ä¸ºé»˜è®¤å€¼"""
    state.current_params = state.default_params.copy()
    return (
        state.default_params['max_new_tokens'],
        state.default_params['temperature'],
        state.default_params['top_p'],
        state.default_params['repetition_penalty'],
        state.default_params['max_history'],
        state.default_params.get('top_k', 50),
        state.default_params.get('do_sample', True),
        "âœ… å‚æ•°å·²é‡ç½®ä¸ºé»˜è®¤å€¼"
    )

# æ¨¡å‹é…ç½®ç®¡ç†
def add_model_path(model_name, model_path):
    """æ·»åŠ æ¨¡å‹è·¯å¾„"""
    if model_name and model_path:
        MODEL_PATHS[model_name] = model_path
        save_models_config(MODEL_PATHS)
        return f"âœ… å·²æ·»åŠ æ¨¡å‹: {model_name}", gr.Dropdown(choices=list(MODEL_PATHS.keys()))
    return "âŒ æ¨¡å‹åç§°å’Œè·¯å¾„ä¸èƒ½ä¸ºç©º", gr.Dropdown(choices=list(MODEL_PATHS.keys()))

def remove_model_path(model_name):
    """ç§»é™¤æ¨¡å‹è·¯å¾„"""
    if model_name in MODEL_PATHS:
        del MODEL_PATHS[model_name]
        save_models_config(MODEL_PATHS)
        return f"âœ… å·²ç§»é™¤æ¨¡å‹: {model_name}", gr.Dropdown(choices=list(MODEL_PATHS.keys()))
    return "âŒ æ¨¡å‹ä¸å­˜åœ¨", gr.Dropdown(choices=list(MODEL_PATHS.keys()))

# å¼ºåˆ¶å†…å­˜æ¸…ç†
def force_clean_memory():
    """å¼ºåˆ¶æ¸…ç†å†…å­˜"""
    try:
        # å¼ºåˆ¶åƒåœ¾å›æ”¶
        gc.collect()
        
        # æ¸…ç†GPUå†…å­˜
        if torch.cuda.is_available():
            before_allocated = torch.cuda.memory_allocated() / (1024**3)
            before_reserved = torch.cuda.memory_reserved() / (1024**3)
            
            # æ¸…ç©ºç¼“å­˜
            torch.cuda.empty_cache()
            
            # å°è¯•é‡ç½®è®¾å¤‡
            try:
                for i in range(torch.cuda.device_count()):
                    torch.cuda.synchronize(i)
                    torch.cuda.empty_cache()
            except:
                pass
            
            after_allocated = torch.cuda.memory_allocated() / (1024**3)
            after_reserved = torch.cuda.memory_reserved() / (1024**3)
            
            logger.info(f"å†…å­˜æ¸…ç†å®Œæˆ - å·²åˆ†é…: {before_allocated:.2f}GB -> {after_allocated:.2f}GB, "
                       f"ä¿ç•™: {before_reserved:.2f}GB -> {after_reserved:.2f}GB")
            
            return f"ğŸ§¹ å†…å­˜æ¸…ç†å®Œæˆï¼é‡Šæ”¾äº† {before_allocated - after_allocated:.2f}GB æ˜¾å­˜"
        else:
            return "ğŸ§¹ CPUå†…å­˜æ¸…ç†å®Œæˆ"
            
    except Exception as e:
        logger.error(f"å†…å­˜æ¸…ç†å¤±è´¥: {e}")
        return f"âŒ å†…å­˜æ¸…ç†å¤±è´¥: {str(e)}"

# åˆå§‹åŒ–åº”ç”¨
def initialize_app():
    """åˆå§‹åŒ–åº”ç”¨"""
    try:
        # åŠ è½½å¯¹è¯
        load_conversations()
        
        # åŠ è½½é…ç½®
        config = load_config()
        
        # åˆ›å»ºé»˜è®¤å¯¹è¯ï¼ˆå¦‚æœæ²¡æœ‰å¯¹è¯ï¼‰
        if not state.conversations:
            create_new_conversation()
        else:
            # æ¢å¤ä¸Šæ¬¡å¯¹è¯æˆ–ä½¿ç”¨ç¬¬ä¸€ä¸ªå¯¹è¯
            state.current_conversation_id = config.get("last_conversation") or list(state.conversations.keys())[0]
        
        # è‡ªåŠ¨åŠ è½½ä¸Šæ¬¡ä½¿ç”¨çš„æ¨¡å‹
        last_model = config.get("last_model")
        if last_model and last_model in MODEL_PATHS:
            logger.info(f"è‡ªåŠ¨åŠ è½½ä¸Šæ¬¡ä½¿ç”¨çš„æ¨¡å‹: {last_model}")
            load_model(MODEL_PATHS[last_model], last_model)
        
        logger.info("åº”ç”¨åˆå§‹åŒ–å®Œæˆ")
    except Exception as e:
        logger.error(f"åº”ç”¨åˆå§‹åŒ–å¤±è´¥: {e}")

# å·¥å…·å‡½æ•°
def get_conversation_dropdown_options():
    """è·å–å¯¹è¯ä¸‹æ‹‰é€‰é¡¹"""
    options = []
    for conv_id, conversation in sorted(
        state.conversations.items(),
        key=lambda x: x[1].get("updated_at", ""),
        reverse=True
    ):
        title = conversation["title"]
        message_count = len(conversation["messages"]) // 2
        display_text = f"{title} ({message_count}æ¡æ¶ˆæ¯)"
        options.append((display_text, conv_id))
    return options

def get_current_conversation_info():
    """è·å–å½“å‰å¯¹è¯ä¿¡æ¯"""
    if state.current_conversation_id in state.conversations:
        conv = state.conversations[state.current_conversation_id]
        model_info = f" | æ¨¡å‹: {conv.get('model_used', 'æœªè®°å½•')}" if conv.get('model_used') else ""
        return f"### ğŸ“ å½“å‰å¯¹è¯: {conv['title']} ({len(conv['messages'])//2} æ¡æ¶ˆæ¯{model_info})"
    return "### ğŸ“ å½“å‰å¯¹è¯: æ— "

# åˆ›å»ºGradioç•Œé¢
def create_interface():
    """åˆ›å»ºGradioç•Œé¢"""
    with gr.Blocks(
        theme=gr.themes.Soft(
            primary_hue="blue",
            secondary_hue="slate"
        ),
        title="å¤§æ¨¡å‹å¯¹è¯ç³»ç»Ÿ",
        css="""
        .gradio-container {
            max-width: 95% !important;
        }
        .conversation-item {
            padding: 8px 12px;
            margin: 2px 0;
            border-radius: 6px;
            cursor: pointer;
        }
        .conversation-item:hover {
            background: rgba(0,0,0,0.05);
        }
        .conversation-active {
            background: rgba(59, 130, 246, 0.1);
            border-left: 3px solid #3b82f6;
        }
        .warning-text {
            color: #e74c3c;
            font-size: 0.9em;
        }
        .success-text {
            color: #27ae60;
            font-size: 0.9em;
        }
        """
    ) as demo:
        gr.Markdown("""
        # ğŸ¤– å¤§æ¨¡å‹å¯¹è¯ç³»ç»Ÿ
        **åŸºäº Qwen ç³»åˆ—æ¨¡å‹çš„æ™ºèƒ½å¯¹è¯å¹³å°**
        """)
        
        # æ˜¾ç¤ºCUDAçŠ¶æ€
        cuda_status = check_cuda_availability()
        if cuda_status['available']:
            gr.Markdown(f"### ğŸ® GPUçŠ¶æ€: âœ… å¯ç”¨ - {cuda_status['device_name']} ({cuda_status['device_count']}ä¸ªGPU)")
        else:
            gr.Markdown("### ğŸ® GPUçŠ¶æ€: âŒ ä¸å¯ç”¨ - å°†ä½¿ç”¨CPUè¿è¡Œ")
        
        # åˆå§‹åŒ–å¯¹è¯é€‰é¡¹
        conversation_options = get_conversation_dropdown_options()
        current_conversation_info = get_current_conversation_info()
        
        with gr.Row(equal_height=False):
            # å·¦ä¾§æ§åˆ¶é¢æ¿
            with gr.Column(scale=1, min_width=300):
                # å¯¹è¯ç®¡ç†åŒºåŸŸ
                with gr.Group():
                    gr.Markdown("### ğŸ’¬ å¯¹è¯ç®¡ç†")
                    with gr.Row():
                        new_convo_btn = gr.Button("ğŸ†• æ–°å»ºå¯¹è¯", variant="primary", size="sm")
                        refresh_convos_btn = gr.Button("ğŸ”„ åˆ·æ–°", variant="secondary", size="sm")
                    
                    # å¯¹è¯é€‰æ‹©ä¸‹æ‹‰èœå•
                    conversation_dropdown = gr.Dropdown(
                        choices=[opt[0] for opt in conversation_options],
                        value=conversation_options[0][0] if conversation_options else "",
                        label="é€‰æ‹©å¯¹è¯",
                        interactive=True,
                        filterable=True
                    )
                    conversation_state = gr.State(state.current_conversation_id)
                    
                    # å¯¹è¯æ“ä½œæŒ‰é’®
                    with gr.Row():
                        delete_convo_btn = gr.Button("ğŸ—‘ï¸ åˆ é™¤å¯¹è¯", variant="stop", size="sm")
                        export_convo_btn = gr.Button("ğŸ“¤ å¯¼å‡ºå¯¹è¯", variant="secondary", size="sm")
                
                # æ¨¡å‹æ§åˆ¶åŒºåŸŸ
                with gr.Group():
                    gr.Markdown("### ğŸš€ æ¨¡å‹æ§åˆ¶")
                    model_dropdown = gr.Dropdown(
                        choices=list(MODEL_PATHS.keys()),
                        value=state.current_model or (list(MODEL_PATHS.keys())[0] if MODEL_PATHS else ""),
                        label="é€‰æ‹©æ¨¡å‹",
                        filterable=True
                    )
                    
                    with gr.Row():
                        load_btn = gr.Button("ğŸ”„ åŠ è½½æ¨¡å‹", variant="primary", scale=1)
                        unload_btn = gr.Button("ğŸ—‘ï¸ å¸è½½æ¨¡å‹", variant="secondary", scale=1)
                    
                    load_status = gr.Markdown("ğŸ‘† è¯·é€‰æ‹©å¹¶åŠ è½½æ¨¡å‹")
                    model_info_html = gr.HTML()
                    
                    # æ¨¡å‹ç®¡ç†æ‰©å±•
                    with gr.Accordion("ğŸ”§ æ¨¡å‹ç®¡ç†", open=False):
                        with gr.Row():
                            new_model_name = gr.Textbox(label="æ¨¡å‹åç§°", placeholder="è¾“å…¥æ¨¡å‹æ˜¾ç¤ºåç§°")
                            new_model_path = gr.Textbox(label="æ¨¡å‹è·¯å¾„", placeholder="è¾“å…¥æ¨¡å‹æœ¬åœ°è·¯å¾„")
                        
                        with gr.Row():
                            add_model_btn = gr.Button("â• æ·»åŠ æ¨¡å‹", size="sm")
                            remove_model_btn = gr.Button("â– ç§»é™¤æ¨¡å‹", size="sm")
                
                # å‚æ•°è®¾ç½®åŒºåŸŸ
                with gr.Group():
                    gr.Markdown("### âš™ï¸ ç”Ÿæˆå‚æ•°")
                    
                    with gr.Row():
                        max_new_tokens = gr.Slider(512, 8192, value=state.current_params['max_new_tokens'], step=256, label="ç”Ÿæˆé•¿åº¦")
                        temperature = gr.Slider(0.1, 2.0, value=state.current_params['temperature'], step=0.1, label="æ¸©åº¦")
                    
                    with gr.Row():
                        top_p = gr.Slider(0.1, 1.0, value=state.current_params['top_p'], step=0.1, label="Top-P")
                        top_k = gr.Slider(1, 100, value=state.current_params.get('top_k', 50), step=1, label="Top-K")
                    
                    with gr.Row():
                        repetition_penalty = gr.Slider(1.0, 2.0, value=state.current_params['repetition_penalty'], step=0.1, label="é‡å¤æƒ©ç½š")
                        max_history_slider = gr.Slider(1, 50, value=state.current_params['max_history'], step=1, label="å¯¹è¯è®°å¿†è½®æ•°")
                    
                    do_sample = gr.Checkbox(value=state.current_params.get('do_sample', True), label="éšæœºé‡‡æ ·")
                    
                    with gr.Row():
                        update_btn = gr.Button("ğŸ’¾ ä¿å­˜å‚æ•°", variant="primary", scale=1)
                        reset_btn = gr.Button("ğŸ”„ é‡ç½®é»˜è®¤", variant="secondary", scale=1)
                    
                    param_status = gr.Markdown("âœ… å‚æ•°å·²å°±ç»ª")
                
                # ç³»ç»Ÿä¿¡æ¯åŒºåŸŸ
                with gr.Group():
                    gr.Markdown("### ğŸ“Š ç³»ç»ŸçŠ¶æ€")
                    with gr.Row():
                        refresh_sys_btn = gr.Button("ğŸ”„ åˆ·æ–°çŠ¶æ€", variant="secondary", size="sm")
                        stop_btn = gr.Button("â¹ï¸ åœæ­¢ç”Ÿæˆ", variant="stop", size="sm")
                        clean_memory_btn = gr.Button("ğŸ§¹ æ¸…ç†å†…å­˜", variant="secondary", size="sm")
                    
                    system_info_html = gr.HTML(get_system_info_html())
                    stats_html = gr.HTML(get_stats_html())
            
            # å³ä¾§èŠå¤©åŒºåŸŸ
            with gr.Column(scale=2, min_width=500):
                current_conversation_display = gr.Markdown(current_conversation_info)
                
                chatbot = gr.Chatbot(
                    value=get_conversation_history(state.current_conversation_id),
                    label="ğŸ’¬ æ™ºèƒ½å¯¹è¯",
                    height=500,
                    type="tuples",
                    show_copy_button=True,
                    avatar_images=(
                        "https://cdn-icons-png.flaticon.com/512/149/149071.png",  # ç”¨æˆ·å¤´åƒ
                        "https://cdn-icons-png.flaticon.com/512/4712/4712035.png"  # åŠ©æ‰‹å¤´åƒ
                    ),
                    placeholder="å¯¹è¯è®°å½•å°†æ˜¾ç¤ºåœ¨è¿™é‡Œ..."
                )
                
                with gr.Row():
                    msg = gr.Textbox(
                        label="",
                        placeholder="è¯·è¾“å…¥æ‚¨çš„é—®é¢˜æˆ–æŒ‡ä»¤...ï¼ˆæŒ‰Enterå‘é€ï¼ŒShift+Enteræ¢è¡Œï¼‰",
                        lines=2,
                        max_lines=5,
                        scale=4,
                        show_label=False
                    )
                    submit_btn = gr.Button("ğŸš€ å‘é€", variant="primary", scale=1)
                
                # å¿«æ·æ“ä½œæŒ‰é’®
                with gr.Row():
                    gr.Markdown("**ğŸ’¡ å¿«æ·æ“ä½œ:**")
                    quick_clear = gr.Button("ğŸ—‘ï¸ æ¸…ç©ºå½“å‰", size="sm")
                    quick_example1 = gr.Button("ğŸ‘‹ æ‰“ä¸ªæ‹›å‘¼", size="sm")
                    quick_example2 = gr.Button("ğŸ“ å†™æ®µä»£ç ", size="sm")
                    quick_example3 = gr.Button("ğŸ¤” è§£é‡Šæ¦‚å¿µ", size="sm")
                
                # æ¶ˆæ¯è®¡æ•°å™¨
                message_counter = gr.HTML(f"""
                <div style="text-align: right; font-size: 0.8em; color: #666; margin-top: 5px;">
                    å½“å‰å¯¹è¯: {len(state.conversations.get(state.current_conversation_id, {}).get('messages', []))//2} æ¡æ¶ˆæ¯
                </div>
                """)
        
        # äº‹ä»¶å¤„ç†å‡½æ•°
        def handle_load_model(model_name):
            if model_name in MODEL_PATHS:
                model_path = MODEL_PATHS[model_name]
                load_result, model_html = load_model(model_path, model_name)
                return load_result, model_html, get_stats_html(), get_system_info_html()
            return "âŒ è¯·é€‰æ‹©æœ‰æ•ˆçš„æ¨¡å‹", "", get_stats_html(), get_system_info_html()
        
        def handle_unload_model():
            unload_result, model_html = unload_model()
            return unload_result, model_html, get_stats_html(), get_system_info_html()
        
        def handle_submit(message, conversation_id):
            if not message.strip():
                yield get_conversation_history(conversation_id), get_stats_html(), get_system_info_html(), "", get_current_conversation_info()
                return
                
            for updated_history, stats_html_content, system_html_content in chat_stream(message, conversation_id):
                yield updated_history, stats_html_content, system_html_content, "", get_current_conversation_info()
        
        def handle_new_conversation():
            new_id = create_new_conversation()
            # ä¿å­˜é…ç½®
            config = load_config()
            config["last_conversation"] = new_id
            save_config(config)
            
            # æ›´æ–°ä¸‹æ‹‰é€‰é¡¹
            options = get_conversation_dropdown_options()
            new_conv_title = f"æ–°å¯¹è¯ (0æ¡æ¶ˆæ¯)"
            
            return (
                gr.Dropdown(choices=[opt[0] for opt in options], value=new_conv_title),
                get_conversation_history(new_id),
                get_current_conversation_info(),
                new_id
            )
        
        def handle_refresh_conversations():
            load_conversations()
            options = get_conversation_dropdown_options()
            current_title = None
            for title, conv_id in options:
                if conv_id == state.current_conversation_id:
                    current_title = title
                    break
            
            return gr.Dropdown(
                choices=[opt[0] for opt in options], 
                value=current_title or (options[0][0] if options else "")
            )
        
        def handle_conversation_change(selected_title):
            """å¤„ç†å¯¹è¯åˆ‡æ¢"""
            # æ ¹æ®æ ‡é¢˜æ‰¾åˆ°å¯¹åº”çš„å¯¹è¯ID
            for title, conv_id in get_conversation_dropdown_options():
                if title == selected_title:
                    state.current_conversation_id = conv_id
                    # ä¿å­˜é…ç½®
                    config = load_config()
                    config["last_conversation"] = conv_id
                    save_config(config)
                    return (
                        get_conversation_history(conv_id),
                        get_current_conversation_info(),
                        conv_id
                    )
            return gr.update(), gr.update(), state.current_conversation_id
        
        def handle_delete_conversation():
            """åˆ é™¤å½“å‰å¯¹è¯"""
            if state.current_conversation_id:
                success = delete_conversation(state.current_conversation_id)
                if success:
                    options = get_conversation_dropdown_options()
                    current_title = options[0][0] if options else "æ–°å¯¹è¯"
                    return (
                        gr.Dropdown(choices=[opt[0] for opt in options], value=current_title),
                        get_conversation_history(state.current_conversation_id),
                        get_current_conversation_info(),
                        state.current_conversation_id,
                        "âœ… å¯¹è¯å·²åˆ é™¤"
                    )
            return gr.update(), gr.update(), gr.update(), gr.update(), "âŒ åˆ é™¤å¯¹è¯å¤±è´¥"
        
        def handle_export_conversation():
            """å¯¼å‡ºå½“å‰å¯¹è¯"""
            if state.current_conversation_id in state.conversations:
                conversation = state.conversations[state.current_conversation_id]
                export_data = {
                    "title": conversation["title"],
                    "model_used": conversation.get("model_used", "æœªçŸ¥"),
                    "created_at": conversation["created_at"],
                    "messages": conversation["messages"]
                }
                filename = f"{conversation['title']}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
                return gr.File(value=json.dumps(export_data, ensure_ascii=False, indent=2), label=filename, visible=True)
            return gr.File(visible=False)
        
        def handle_clear_current_conversation():
            if state.current_conversation_id in state.conversations:
                state.conversations[state.current_conversation_id]["messages"] = []
                save_conversation(state.current_conversation_id)
                return [], get_current_conversation_info()
            return [], get_current_conversation_info()
        
        def handle_quick_example(example_type):
            examples = {
                "greeting": "ä½ å¥½ï¼è¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚",
                "code": "è¯·ç”¨Pythonå†™ä¸€ä¸ªå¿«é€Ÿæ’åºç®—æ³•ï¼Œå¹¶æ·»åŠ è¯¦ç»†æ³¨é‡Šã€‚",
                "explain": "è¯·ç”¨é€šä¿—æ˜“æ‡‚çš„æ–¹å¼è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ã€‚"
            }
            return examples.get(example_type, "ä½ å¥½ï¼")
        
        # ç»‘å®šäº‹ä»¶
        load_btn.click(
            fn=handle_load_model,
            inputs=[model_dropdown],
            outputs=[load_status, model_info_html, stats_html, system_info_html]
        )
        
        unload_btn.click(
            fn=handle_unload_model,
            outputs=[load_status, model_info_html, stats_html, system_info_html]
        )
        
        # æäº¤æ¶ˆæ¯
        submit_event = msg.submit(
            fn=handle_submit,
            inputs=[msg, conversation_state],
            outputs=[chatbot, stats_html, system_info_html, msg, current_conversation_display]
        )
        
        submit_btn.click(
            fn=handle_submit,
            inputs=[msg, conversation_state],
            outputs=[chatbot, stats_html, system_info_html, msg, current_conversation_display]
        )
        
        stop_btn.click(
            fn=stop_generation,
            outputs=[load_status]
        )
        
        # å†…å­˜æ¸…ç†
        clean_memory_btn.click(
            fn=force_clean_memory,
            outputs=[load_status]
        )
        
        # å¯¹è¯ç®¡ç†
        new_convo_btn.click(
            fn=handle_new_conversation,
            outputs=[conversation_dropdown, chatbot, current_conversation_display, conversation_state]
        )
        
        refresh_convos_btn.click(
            fn=handle_refresh_conversations,
            outputs=[conversation_dropdown]
        )
        
        delete_convo_btn.click(
            fn=handle_delete_conversation,
            outputs=[conversation_dropdown, chatbot, current_conversation_display, conversation_state, load_status]
        )
        
        export_convo_btn.click(
            fn=handle_export_conversation,
            outputs=[gr.File(visible=True)]
        )
        
        # å¯¹è¯åˆ‡æ¢äº‹ä»¶
        conversation_dropdown.change(
            fn=handle_conversation_change,
            inputs=[conversation_dropdown],
            outputs=[chatbot, current_conversation_display, conversation_state]
        )
        
        # æ¨¡å‹ç®¡ç†
        add_model_btn.click(
            fn=add_model_path,
            inputs=[new_model_name, new_model_path],
            outputs=[load_status, model_dropdown]
        )
        
        remove_model_btn.click(
            fn=remove_model_path,
            inputs=[model_dropdown],
            outputs=[load_status, model_dropdown]
        )
        
        # å‚æ•°ç®¡ç†
        update_btn.click(
            fn=update_params,
            inputs=[max_new_tokens, temperature, top_p, repetition_penalty, max_history_slider, top_k, do_sample],
            outputs=[param_status]
        )
        
        reset_btn.click(
            fn=reset_params,
            outputs=[max_new_tokens, temperature, top_p, repetition_penalty, max_history_slider, top_k, do_sample, param_status]
        )
        
        # ç³»ç»ŸçŠ¶æ€
        refresh_sys_btn.click(
            fn=lambda: [get_system_info_html(), get_stats_html()],
            outputs=[system_info_html, stats_html]
        )
        
        # å¿«æ·æ“ä½œ
        quick_clear.click(
            fn=handle_clear_current_conversation,
            outputs=[chatbot, current_conversation_display]
        )
        
        quick_example1.click(
            fn=lambda: handle_quick_example("greeting"),
            outputs=[msg]
        )
        
        quick_example2.click(
            fn=lambda: handle_quick_example("code"),
            outputs=[msg]
        )
        
        quick_example3.click(
            fn=lambda: handle_quick_example("explain"),
            outputs=[msg]
        )
        
        return demo

# å¯åŠ¨åº”ç”¨
if __name__ == "__main__":
    # åˆå§‹åŒ–åº”ç”¨
    initialize_app()
    
    # åˆ›å»ºç•Œé¢
    demo = create_interface()
    
    # å¯åŠ¨æœåŠ¡
    try:
        demo.queue(
            max_size=20,
            api_open=False
        ).launch(
            server_name="0.0.0.0",
            server_port=7860,
            share=False,
            inbrowser=True,
            show_error=True,
            quiet=False,
            debug=False
        )
    except Exception as e:
        logger.error(f"å¯åŠ¨åº”ç”¨å¤±è´¥: {e}")
        print(f"å¯åŠ¨å¤±è´¥: {e}")