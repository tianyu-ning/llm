# app.py - ä¼˜åŒ–ç‰ˆ
import gradio as gr
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer
import time
import psutil
try:
    import GPUtil
except Exception:
    GPUtil = None
import os
from datetime import datetime
import threading
import re
import gc
import json
import uuid
import shutil
from pathlib import Path
import logging
from typing import Dict, List, Optional, Tuple, Any
from logging.handlers import RotatingFileHandler
from functools import lru_cache
import tempfile

# è®¾ç½®PyTorchä¸»çº¿ç¨‹æ•°ï¼ˆå¯æ ¹æ®ç¯å¢ƒè°ƒæ•´ï¼‰
torch.set_num_threads(4)
torch.set_num_interop_threads(1)

# æ—¥å¿—é…ç½®ï¼šä½¿ç”¨æ—‹è½¬æ—¥å¿—é¿å…æ— é™å¢é•¿
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
file_handler = RotatingFileHandler("app.log", maxBytes=5 * 1024 * 1024, backupCount=3, encoding='utf-8')
file_handler.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))
stream_handler = logging.StreamHandler()
stream_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))
logger.addHandler(file_handler)
logger.addHandler(stream_handler)

# å¸¸é‡é…ç½®
CONFIG = {
    "conversations_dir": "conversations",
    "models_dir": "models",
    "config_file": "app_config.json",
    "models_config_file": "models_config.json",
    "max_conversations": 100,
    "max_message_length": 8000,
    "default_timeout": 120,
    "max_input_tokens": 6000,
    "reserved_tokens": 500,
    # å®‰å…¨é¡¹ï¼šæ˜¯å¦å…è®¸ä»è¿œç¨‹æ¨¡å‹ä»“åº“æ‰§è¡Œä»»æ„ä»£ç ï¼ˆé»˜è®¤å…³é—­ï¼‰
    "allow_trust_remote_code": False
}

# Ensure directories exist
Path(CONFIG["conversations_dir"]).mkdir(exist_ok=True)
Path(CONFIG["models_dir"]).mkdir(exist_ok=True)

# æ£€æŸ¥ CUDA å¯ç”¨æ€§
def check_cuda_availability():
    """æ£€æŸ¥ CUDA å¯ç”¨æ€§å¹¶è¿”å›è¯¦ç»†ä¿¡æ¯"""
    cuda_info = {
        'available': torch.cuda.is_available(),
        'device_count': torch.cuda.device_count() if torch.cuda.is_available() else 0,
        'current_device': torch.cuda.current_device() if torch.cuda.is_available() else None,
        'device_name': None,
        'cuda_version': torch.version.cuda if torch.cuda.is_available() else None
    }

    if cuda_info['available'] and cuda_info['device_count'] > 0:
        try:
            cuda_info['device_name'] = torch.cuda.get_device_name(0)
            if torch.cuda.is_available():
                try:
                    torch.cuda.init()
                except Exception:
                    pass
                cuda_info['memory_allocated'] = torch.cuda.memory_allocated(0) / (1024**3)
                cuda_info['memory_reserved'] = torch.cuda.memory_reserved(0) / (1024**3)
                cuda_info['total_memory'] = torch.cuda.get_device_properties(0).total_memory / (1024**3)
        except Exception as e:
            logger.error(f"è·å–CUDAè®¾å¤‡ä¿¡æ¯å¤±è´¥: {e}")

    return cuda_info

cuda_info = check_cuda_availability()
logger.info(f"CUDAå¯ç”¨æ€§: {cuda_info['available']}")
logger.info(f"GPUæ•°é‡: {cuda_info['device_count']}")
if cuda_info['available']:
    logger.info(f"GPUåç§°: {cuda_info.get('device_name')}")
    logger.info(f"CUDAç‰ˆæœ¬: {cuda_info.get('cuda_version')}")
    logger.info(f"GPUæ€»å†…å­˜: {cuda_info.get('total_memory', 0):.2f} GB")

# åŠ è½½/ä¿å­˜æ¨¡å‹é…ç½®
def load_models_config():
    default_models = {
        "Qwen3-1.7B": "/Data/llm_modl_data/qwen3-1.7B",
        "Qwen3-4B-Thinking-FP8": "/Data/llm_modl_data/qwen3-4B-Thinking-2507-FP8"
    }
    try:
        if os.path.exists(CONFIG["models_config_file"]):
            with open(CONFIG["models_config_file"], 'r', encoding='utf-8') as f:
                config = json.load(f)
                return config.get("models", default_models)
    except Exception as e:
        logger.error(f"åŠ è½½æ¨¡å‹é…ç½®å¤±è´¥: {e}")
    return default_models

def save_models_config(models):
    # å†™æ–‡ä»¶æ—¶ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶å¹¶æ›¿æ¢ï¼Œå‡å°‘ä¸­æ–­å¯¼è‡´æ–‡ä»¶ç ´åé£é™©
    try:
        tmp_path = CONFIG["models_config_file"] + ".tmp"
        with open(tmp_path, 'w', encoding='utf-8') as f:
            json.dump({"models": models}, f, ensure_ascii=False, indent=2)
        os.replace(tmp_path, CONFIG["models_config_file"])
    except Exception as e:
        logger.error(f"ä¿å­˜æ¨¡å‹é…ç½®å¤±è´¥: {e}")

MODEL_PATHS = load_models_config()

# å…¨å±€çŠ¶æ€
class GlobalState:
    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.current_model = None
        self.current_conversation_id = None
        self.conversations = {}
        self.stats = {
            'total_requests': 0,
            'total_tokens': 0,
            'total_time': 0,
            'start_time': datetime.now(),
            'failed_requests': 0
        }
        self.generation_stop_event = None
        self.is_generating = False
        self.model_lock = threading.Lock()
        self.model_max_length = 8192
        self.default_params = {
            'max_new_tokens': 2048,
            'temperature': 0.7,
            'top_p': 0.9,
            'repetition_penalty': 1.1,
            'max_history': 10,
            'top_k': 50,
            'do_sample': True
        }
        self.current_params = self.default_params.copy()

state = GlobalState()

# é…ç½®ç®¡ç†
def load_config():
    try:
        if os.path.exists(CONFIG["config_file"]):
            with open(CONFIG["config_file"], 'r', encoding='utf-8') as f:
                return json.load(f)
    except Exception as e:
        logger.error(f"åŠ è½½é…ç½®å¤±è´¥: {e}")
    return {"last_model": None, "last_conversation": None}

def save_config(config):
    try:
        tmp_path = CONFIG["config_file"] + ".tmp"
        with open(tmp_path, 'w', encoding='utf-8') as f:
            json.dump(config, f, ensure_ascii=False, indent=2)
        os.replace(tmp_path, CONFIG["config_file"])
    except Exception as e:
        logger.error(f"ä¿å­˜é…ç½®å¤±è´¥: {e}")

# å¯¹è¯ç®¡ç†
def load_conversations():
    state.conversations = {}
    conversations_dir = Path(CONFIG["conversations_dir"])
    for file_path in conversations_dir.glob("*.json"):
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                conversation_data = json.load(f)
                state.conversations[conversation_data["id"]] = conversation_data
        except Exception as e:
            logger.error(f"åŠ è½½å¯¹è¯å¤±è´¥ {file_path}: {e}")

    sorted_convos = sorted(
        state.conversations.values(),
        key=lambda x: x.get("updated_at", ""),
        reverse=True
    )[:CONFIG["max_conversations"]]

    state.conversations = {conv["id"]: conv for conv in sorted_convos}
    logger.info(f"å·²åŠ è½½ {len(state.conversations)} ä¸ªå¯¹è¯")

def save_conversation(conversation_id=None):
    if conversation_id not in state.conversations:
        return
    conversation = state.conversations[conversation_id]
    conversation["updated_at"] = datetime.now().isoformat()
    file_path = Path(CONFIG["conversations_dir"]) / f"{conversation_id}.json"
    try:
        tmp = f"{file_path}.tmp"
        with open(tmp, 'w', encoding='utf-8') as f:
            json.dump(conversation, f, ensure_ascii=False, indent=2)
        os.replace(tmp, file_path)
    except Exception as e:
        logger.error(f"ä¿å­˜å¯¹è¯å¤±è´¥: {e}")

def create_new_conversation(title="æ–°å¯¹è¯"):
    conversation_id = str(uuid.uuid4())
    state.conversations[conversation_id] = {
        "id": conversation_id,
        "title": title,
        "messages": [],
        "created_at": datetime.now().isoformat(),
        "updated_at": datetime.now().isoformat(),
        "model_used": state.current_model
    }
    state.current_conversation_id = conversation_id
    save_conversation(conversation_id)
    logger.info(f"åˆ›å»ºæ–°å¯¹è¯: {title}")
    return conversation_id

def delete_conversation(conversation_id):
    if conversation_id in state.conversations:
        title = state.conversations[conversation_id]["title"]
        del state.conversations[conversation_id]
        file_path = Path(CONFIG["conversations_dir"]) / f"{conversation_id}.json"
        if file_path.exists():
            file_path.unlink()
        if state.current_conversation_id == conversation_id:
            if state.conversations:
                state.current_conversation_id = list(state.conversations.keys())[0]
            else:
                create_new_conversation()
        logger.info(f"åˆ é™¤å¯¹è¯: {title}")
        return True
    return False

def get_conversation_history(conversation_id):
    if conversation_id not in state.conversations:
        return []
    messages = state.conversations[conversation_id]["messages"]
    history = []
    for i in range(0, len(messages), 2):
        if i + 1 < len(messages):
            history.append([messages[i]["content"], messages[i + 1]["content"]])
    return history

# æ–‡æœ¬å¤„ç†
def clean_text(text):
    if not text:
        return ""
    try:
        cleaned = re.sub(r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F]', '', text)
        return cleaned
    except Exception:
        return re.sub(r'[^\x00-\x7F\u4e00-\u9fff]+', '', text)

def truncate_text(text, max_length=CONFIG["max_message_length"]):
    if len(text) > max_length:
        return text[:max_length] + "...(å·²æˆªæ–­)"
    return text

# Token count cache helpers
@lru_cache(maxsize=8192)
def _token_len_cached(tokenizer_id: int, text: str) -> int:
    try:
        if state.tokenizer is None:
            return max(1, len(text) // 4)
        return len(state.tokenizer.encode(text, add_special_tokens=False))
    except Exception:
        return max(1, len(text) // 4)

def count_tokens(text: str) -> int:
    tid = id(state.tokenizer) if state.tokenizer is not None else 0
    # normalize text to str
    if text is None:
        return 0
    return _token_len_cached(tid, text)

def clear_token_cache():
    try:
        _token_len_cached.cache_clear()
    except Exception:
        pass

# ç³»ç»Ÿç›‘æ§
def get_system_info():
    info = {}
    try:
        info['cpu_usage'] = psutil.cpu_percent(interval=0.1)
        info['memory_usage'] = psutil.virtual_memory().percent
        info['memory_used_gb'] = psutil.virtual_memory().used / (1024**3)
        info['memory_total_gb'] = psutil.virtual_memory().total / (1024**3)

        info['cuda_available'] = torch.cuda.is_available()
        info['cuda_device_count'] = torch.cuda.device_count() if torch.cuda.is_available() else 0

        if info['cuda_available'] and info['cuda_device_count'] > 0:
            try:
                info['torch_gpu_name'] = torch.cuda.get_device_name(0)
                info['torch_gpu_memory_allocated'] = torch.cuda.memory_allocated(0) / (1024**3)
                info['torch_gpu_memory_reserved'] = torch.cuda.memory_reserved(0) / (1024**3)
                info['torch_gpu_total_memory'] = torch.cuda.get_device_properties(0).total_memory / (1024**3)

                if GPUtil is not None:
                    try:
                        gpus = GPUtil.getGPUs()
                        if gpus:
                            gpu = gpus[0]
                            info['gpu_name'] = gpu.name
                            info['gpu_usage'] = gpu.load * 100
                            info['gpu_memory_used'] = gpu.memoryUsed
                            info['gpu_memory_total'] = gpu.memoryTotal
                            info['gpu_temperature'] = gpu.temperature
                    except Exception as e:
                        logger.warning(f"GPUtil è·å–GPUä¿¡æ¯å¤±è´¥: {e}")
                else:
                    info['gpu_name'] = info.get('torch_gpu_name')
                    info['gpu_usage'] = (info.get('torch_gpu_memory_allocated', 0) / max(info.get('torch_gpu_total_memory', 1), 1)) * 100
                    info['gpu_memory_used'] = info.get('torch_gpu_memory_allocated', 0) * 1024
                    info['gpu_memory_total'] = info.get('torch_gpu_total_memory', 0) * 1024
                    info['gpu_temperature'] = 0
            except Exception as e:
                logger.warning(f"è·å–GPUä¿¡æ¯å¤±è´¥: {e}")
                info['cuda_available'] = False

        disk_usage = psutil.disk_usage('/')
        info['disk_usage'] = disk_usage.percent
        info['disk_free_gb'] = disk_usage.free / (1024**3)
    except Exception as e:
        logger.error(f"è·å–ç³»ç»Ÿä¿¡æ¯å¤±è´¥: {e}")
    return info

def get_system_info_html():
    system_info = get_system_info()
    html = """
    <div style="background: linear-gradient(135deg, #4facfe 0%, #00f2fe 100%); color: white; padding: 15px; border-radius: 10px; margin: 10px 0;">
        <h3 style="margin-top: 0; margin-bottom: 15px;">ğŸ’» ç³»ç»Ÿç›‘æ§</h3>
    """
    try:
        html += f"""
            <div style="margin: 10px 0;">
                <div style="display: flex; justify-content: space-between; font-size: 0.9em;">
                    <span>ğŸ–¥ï¸ CPU</span>
                    <span>{system_info.get('cpu_usage', 0):.1f}%</span>
                </div>
                <div style="background: rgba(255,255,255,0.3); border-radius: 10px; height: 6px; margin-top: 5px;">
                    <div style="background: #ff6b6b; width: {system_info.get('cpu_usage', 0)}%; height: 100%; border-radius: 10px;"></div>
                </div>
            </div>
        """
        html += f"""
            <div style="margin: 10px 0;">
                <div style="display: flex; justify-content: space-between; font-size: 0.9em;">
                    <span>ğŸ’¾ å†…å­˜</span>
                    <span>{system_info.get('memory_usage', 0):.1f}%</span>
                </div>
                <div style="background: rgba(255,255,255,0.3); border-radius: 10px; height: 6px; margin-top: 5px;">
                    <div style="background: #4ecdc4; width: {system_info.get('memory_usage', 0)}%; height: 100%; border-radius: 10px;"></div>
                </div>
            </div>
        """
        if system_info.get('cuda_available', False):
            gpu_memory_usage = (system_info.get('torch_gpu_memory_allocated', 0) / max(system_info.get('torch_gpu_total_memory', 1), 1)) * 100
            html += f"""
                <div style="margin: 10px 0;">
                    <div style="display: flex; justify-content: space-between; font-size: 0.9em;">
                        <span>ğŸ® GPUå†…å­˜</span>
                        <span>{gpu_memory_usage:.1f}%</span>
                    </div>
                    <div style="background: rgba(255,255,255,0.3); border-radius: 10px; height: 6px; margin-top: 5px;">
                        <div style="background: #45b7d1; width: {gpu_memory_usage}%; height: 100%; border-radius: 10px;"></div>
                    </div>
                </div>
            """
            html += f"""
                <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 10px; margin-top: 10px;">
                    <div style="text-align: center;">
                        <div style="font-size: 0.9em; font-weight: bold;">{system_info.get('cuda_device_count', 0)}</div>
                        <div style="font-size: 0.7em;">GPUæ•°é‡</div>
                    </div>
                    <div style="text-align: center;">
                        <div style="font-size: 0.9em; font-weight: bold;">{system_info.get('torch_gpu_memory_allocated', 0):.1f}G</div>
                        <div style="font-size: 0.7em;">å·²ç”¨æ˜¾å­˜</div>
                    </div>
                </div>
                <div style="margin-top: 5px; font-size: 0.7em; text-align: center;">
                    {system_info.get('torch_gpu_name', 'æœªçŸ¥GPU')}
                </div>
            """
        else:
            html += """
                <div style="margin: 10px 0; text-align: center; color: #ff6b6b;">
                    âš ï¸ CUDAä¸å¯ç”¨ï¼Œæ¨¡å‹è¿è¡Œåœ¨CPUä¸Š
                </div>
            """
        html += f"""
            <div style="margin: 10px 0;">
                <div style="display: flex; justify-content: space-between; font-size: 0.9em;">
                    <span>ğŸ’¾ ç£ç›˜</span>
                    <span>{system_info.get('disk_usage', 0):.1f}%</span>
                </div>
                <div style="background: rgba(255,255,255,0.3); border-radius: 10px; height: 6px; margin-top: 5px;">
                    <div style="background: #f9c74f; width: {system_info.get('disk_usage', 0)}%; height: 100%; border-radius: 10px;"></div>
                </div>
            </div>
        """
    except Exception as e:
        logger.error(f"ç”Ÿæˆç³»ç»Ÿä¿¡æ¯HTMLå¤±è´¥: {e}")
        html += "<div style='color: #ff6b6b;'>ç³»ç»Ÿä¿¡æ¯è·å–å¤±è´¥</div>"
    html += "</div>"
    return html

def get_stats_html():
    try:
        run_time = datetime.now() - state.stats['start_time']
        hours = run_time.total_seconds() // 3600
        minutes = (run_time.total_seconds() % 3600) // 60
        avg_time = state.stats['total_time'] / max(state.stats['total_requests'], 1)
        current_conv = state.conversations.get(state.current_conversation_id, {})
        success_rate = 100
        if state.stats['total_requests'] > 0:
            success_rate = ((state.stats['total_requests'] - state.stats['failed_requests']) / state.stats['total_requests']) * 100
        device_info = "CPU"
        if state.model is not None:
            device_info = str(state.model.device)
            if 'cuda' in device_info:
                device_info = f"GPU:{device_info.split(':')[-1]}"
        html = f"""
        <div style="background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%); color: white; padding: 15px; border-radius: 10px; margin: 10px 0;">
            <h3 style="margin-top: 0; margin-bottom: 15px;">ğŸ“Š ä½¿ç”¨ç»Ÿè®¡</h3>
            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 10px;">
                <div style="text-align: center;">
                    <div style="font-size: 1.3em; font-weight: bold;">{state.stats['total_requests']}</div>
                    <div style="font-size: 0.8em;">è¯·æ±‚æ•°</div>
                </div>
                <div style="text-align: center;">
                    <div style="font-size: 1.3em; font-weight: bold;">{state.stats['total_tokens']}</div>
                    <div style="font-size: 0.8em;">Tokenæ•°</div>
                </div>
                <div style="text-align: center;">
                    <div style="font-size: 1.3em; font-weight: bold;">{avg_time:.1f}s</div>
                    <div style="font-size: 0.8em;">å¹³å‡å“åº”</div>
                </div>
                <div style="text-align: center;">
                    <div style="font-size: 1.3em; font-weight: bold;">{success_rate:.1f}%</div>
                    <div style="font-size: 0.8em;">æˆåŠŸç‡</div>
                </div>
            </div>
            <div style="text-align: center; margin-top: 10px; font-size: 0.8em;">
                è¿è¡Œ: {int(hours)}h{int(minutes)}m | å¯¹è¯: {len(state.conversations)}<br>
                è®¾å¤‡: {device_info} | æ¨¡å‹: {state.current_model or 'æ— '}
            </div>
        </div>
        """
        return html
    except Exception as e:
        logger.error(f"ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯HTMLå¤±è´¥: {e}")
        return "<div>ç»Ÿè®¡ä¿¡æ¯åŠ è½½å¤±è´¥</div>"

# æ¨¡å‹ä¿¡æ¯å’ŒåŠ è½½
def get_model_info(model_path):
    try:
        model_name = os.path.basename(model_path)
        info = {
            "name": model_name,
            "path": model_path,
            "parameters": "æœªçŸ¥",
            "type": "æœªçŸ¥",
            "size_gb": "æœªçŸ¥"
        }
        if "1.7B" in model_name or "1.7b" in model_name:
            info["parameters"] = "17äº¿"
            info["size_gb"] = "~3.5GB"
        elif "4B" in model_name or "4b" in model_name:
            info["parameters"] = "40äº¿"
            info["size_gb"] = "~8GB"
        elif "7B" in model_name or "7b" in model_name:
            info["parameters"] = "70äº¿"
            info["size_gb"] = "~14GB"
        elif "14B" in model_name or "14b" in model_name:
            info["parameters"] = "140äº¿"
            info["size_gb"] = "~28GB"
        if "Thinking" in model_name:
            info["type"] = "æ€è€ƒå¢å¼ºå‹"
        elif "Chat" in model_name:
            info["type"] = "å¯¹è¯ä¼˜åŒ–å‹"
        elif "Instruct" in model_name:
            info["type"] = "æŒ‡ä»¤è°ƒä¼˜å‹"
        else:
            info["type"] = "åŸºç¡€æ¨¡å‹"
        return info
    except Exception as e:
        logger.error(f"è·å–æ¨¡å‹ä¿¡æ¯å¤±è´¥: {e}")
        return {"name": os.path.basename(model_path), "path": model_path, "parameters": "æœªçŸ¥", "type": "æœªçŸ¥", "size_gb": "æœªçŸ¥"}

def get_model_display_info(model_path):
    info = get_model_info(model_path)
    device_info = "CPU"
    if state.model is not None:
        device_info = str(state.model.device)
    cuda_avail = check_cuda_availability()
    gpu_status = "âœ… å¯ç”¨" if cuda_avail['available'] else "âŒ ä¸å¯ç”¨"
    return f"""
    <div style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 15px; border-radius: 10px; margin: 10px 0;">
        <h3 style="margin-top: 0; margin-bottom: 10px;">ğŸ“ æ¨¡å‹ä¿¡æ¯</h3>
        <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 10px;">
            <div style="text-align: center;">
                <div style="font-size: 1.2em; font-weight: bold;">{info['parameters']}</div>
                <div style="font-size: 0.8em;">å‚æ•°è§„æ¨¡</div>
            </div>
            <div style="text-align: center;">
                <div style="font-size: 1.2em; font-weight: bold;">{info['type']}</div>
                <div style="font-size: 0.8em;">æ¨¡å‹ç±»å‹</div>
            </div>
            <div style="text-align: center;">
                <div style="font-size: 1.2em; font-weight: bold;">{info['size_gb']}</div>
                <div style="font-size: 0.8em;">é¢„ä¼°å¤§å°</div>
            </div>
            <div style="text-align: center;">
                <div style="font-size: 1.2em; font-weight: bold;">{device_info}</div>
                <div style="font-size: 0.8em;">è¿è¡Œè®¾å¤‡</div>
            </div>
        </div>
        <div style="margin-top: 10px; font-size: 0.8em; text-align: center;">
            GPUçŠ¶æ€: {gpu_status} | è·¯å¾„: {os.path.basename(model_path)}
        </div>
    </div>
    """

def load_model_to_cpu(model_path, model_display_name):
    with state.model_lock:
        try:
            logger.info(f"åŠ è½½æ¨¡å‹åˆ°CPU: {model_display_name}")
            trc = CONFIG.get('allow_trust_remote_code', False)
            if trc:
                logger.warning("allow_trust_remote_code ä¸º Trueï¼šå°†ä¿¡ä»»å¹¶æ‰§è¡Œæ¨¡å‹ä»“åº“é‡Œçš„è¿œç¨‹ä»£ç ï¼Œè¯·ç¡®è®¤æ¥æºå¯ä¿¡ã€‚")
            state.tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=trc)
            clear_token_cache()
            state.model = AutoModelForCausalLM.from_pretrained(
                model_path,
                torch_dtype=torch.float32,
                device_map=None,
                low_cpu_mem_usage=True,
                trust_remote_code=trc
            )
            state.model = state.model.to('cpu')
            state.current_model = model_display_name
            config = load_config()
            config["last_model"] = model_display_name
            save_config(config)
            logger.info(f"æ¨¡å‹CPUåŠ è½½æˆåŠŸ: {model_display_name}")
            return f"âœ… **{model_display_name}** å·²åŠ è½½åˆ°CPU", get_model_display_info(model_path)
        except Exception as e:
            logger.error(f"CPUåŠ è½½å¤±è´¥: {e}")
            return f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}", ""

def load_model_with_fallback(model_path, model_display_name):
    with state.model_lock:
        # é¦–å…ˆç¡®ä¿CUDAæ˜¯å¦å¯ç”¨
        cuda_avail = check_cuda_availability()
        if not cuda_avail['available']:
            logger.warning("CUDAä¸å¯ç”¨ï¼Œå°†åŠ è½½æ¨¡å‹åˆ°CPU")
            return load_model_to_cpu(model_path, model_display_name)

        logger.info(f"å°è¯•åŠ è½½æ¨¡å‹åˆ°GPU: {model_display_name}")
        torch.cuda.empty_cache()
        gc.collect()

        trc = CONFIG.get('allow_trust_remote_code', False)
        if trc:
            logger.warning("allow_trust_remote_code ä¸º Trueï¼šå°†ä¿¡ä»»å¹¶æ‰§è¡Œæ¨¡å‹ä»“åº“é‡Œçš„è¿œç¨‹ä»£ç ï¼Œè¯·ç¡®è®¤æ¥æºå¯ä¿¡ã€‚")

        # load tokenizer
        state.tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=trc)
        clear_token_cache()

        # å¤šç§è®¾å¤‡åˆ†é…ç­–ç•¥ï¼Œä¾æ¬¡å°è¯•
        load_exceptions = []
        try:
            logger.info("å°è¯• device_map='auto'")
            state.model = AutoModelForCausalLM.from_pretrained(
                model_path,
                torch_dtype=torch.float16,
                device_map="auto",
                low_cpu_mem_usage=True,
                trust_remote_code=trc
            )
        except Exception as e:
            logger.warning(f"device_map='auto' å¤±è´¥: {e}")
            load_exceptions.append(e)
            try:
                logger.info("å°è¯• device_map=None å¹¶æ‰‹åŠ¨è½¬ç§»åˆ° cuda")
                state.model = AutoModelForCausalLM.from_pretrained(
                    model_path,
                    torch_dtype=torch.float16,
                    device_map=None,
                    low_cpu_mem_usage=True,
                    trust_remote_code=trc
                )
                state.model = state.model.to('cuda')
            except Exception as e2:
                logger.warning(f"device_map=None -> to('cuda') å¤±è´¥: {e2}")
                load_exceptions.append(e2)
                logger.error("æ‰€æœ‰GPUåŠ è½½æ–¹æ³•å¤±è´¥ï¼Œå›é€€åˆ° CPU åŠ è½½")
                return load_model_to_cpu(model_path, model_display_name)

        state.current_model = model_display_name
        # ä¿å­˜æœ€è¿‘ä½¿ç”¨æ¨¡å‹
        config = load_config()
        config["last_model"] = model_display_name
        save_config(config)
        system_info = get_system_info()
        device_info = f"è¿è¡Œè®¾å¤‡: {state.model.device}"
        if system_info.get('cuda_available', False):
            device_info += f" | GPUå†…å­˜å ç”¨: {system_info.get('torch_gpu_memory_allocated', 0):.2f} GB"
        logger.info(f"æ¨¡å‹GPUåŠ è½½æˆåŠŸ: {model_display_name}")
        return f"âœ… **{model_display_name}** åŠ è½½æˆåŠŸï¼\n\n{device_info}", get_model_display_info(model_path)

def load_model(model_path, model_display_name):
    logger.info(f"å¼€å§‹åŠ è½½æ¨¡å‹: {model_display_name}")
    if state.model is not None:
        unload_model()
    if not os.path.exists(model_path):
        return f"âŒ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}", ""
    # å°è¯•gpuåŠ è½½ï¼ˆå†…éƒ¨æœ‰é”ï¼‰
    return load_model_with_fallback(model_path, model_display_name)

def unload_model():
    with state.model_lock:
        if state.model is not None:
            model_name = state.current_model
            if torch.cuda.is_available():
                try:
                    allocated_before = torch.cuda.memory_allocated() / (1024**3)
                    reserved_before = torch.cuda.memory_reserved() / (1024**3)
                    logger.info(f"å¸è½½å‰ - å·²åˆ†é…: {allocated_before:.2f}GB, ä¿ç•™: {reserved_before:.2f}GB")
                except Exception:
                    allocated_before = reserved_before = 0.0

            # å¦‚æœæ­£åœ¨ç”Ÿæˆï¼Œå°è¯•ä¼˜é›…åœæ­¢
            if getattr(state, 'is_generating', False) and state.generation_stop_event:
                logger.info("æ£€æµ‹åˆ°æ­£åœ¨ç”Ÿæˆï¼Œå°è¯•å…ˆåœæ­¢ç”Ÿæˆä»¥ä¾¿å¸è½½æ¨¡å‹")
                try:
                    state.generation_stop_event.set()
                    wait_start = time.time()
                    while state.is_generating and time.time() - wait_start < 5:
                        time.sleep(0.1)
                except Exception:
                    logger.warning("å°è¯•åœæ­¢ç”Ÿæˆæ—¶å‘ç”Ÿé—®é¢˜ï¼Œç»§ç»­å¸è½½æµç¨‹")

            try:
                if hasattr(state.model, 'device') and 'cuda' in str(state.model.device):
                    try:
                        state.model = state.model.cpu()
                        logger.info("æ¨¡å‹å·²ç§»åŠ¨åˆ°CPU")
                        time.sleep(0.5)
                    except Exception as e:
                        logger.warning(f"ç§»åŠ¨æ¨¡å‹åˆ°CPUå¤±è´¥: {e}")

                # åˆ é™¤å¼•ç”¨
                del state.model
                del state.tokenizer
            except Exception as e:
                logger.warning(f"åˆ é™¤æ¨¡å‹å¼•ç”¨æ—¶å‡ºé”™: {e}")
            finally:
                state.model = None
                state.tokenizer = None
                state.current_model = None
                clear_token_cache()

            for i in range(3):
                gc.collect()
                time.sleep(0.1)

            if torch.cuda.is_available():
                try:
                    torch.cuda.empty_cache()
                    for i in range(torch.cuda.device_count()):
                        torch.cuda.synchronize(i)
                    torch.cuda.empty_cache()
                    allocated_after = torch.cuda.memory_allocated() / (1024**3)
                    reserved_after = torch.cuda.memory_reserved() / (1024**3)
                    memory_freed = allocated_before - allocated_after
                    logger.info(f"å¸è½½å - å·²åˆ†é…: {allocated_after:.2f}GB, ä¿ç•™: {reserved_after:.2f}GB")
                    logger.info(f"é‡Šæ”¾å†…å­˜: {memory_freed:.2f}GB")
                    if memory_freed < 0.1:
                        logger.warning("æ¨¡å‹å¸è½½åé‡Šæ”¾çš„å†…å­˜å¾ˆå°‘ï¼Œå¯èƒ½ä»æœ‰å¼•ç”¨æœªæ¸…é™¤")
                except Exception as e:
                    logger.warning(f"é‡ç½®GPUå†…å­˜å¤±è´¥: {e}")

            logger.info(f"æ¨¡å‹å·²å¸è½½: {model_name}")
            return "âœ… æ¨¡å‹å·²å¸è½½", ""
        return "â„¹ï¸ æ²¡æœ‰åŠ è½½çš„æ¨¡å‹", ""

# æ™ºèƒ½æˆªæ–­
def smart_truncate_messages(messages, max_tokens=CONFIG["max_input_tokens"]):
    if not messages:
        return messages

    def count_tokens_for_messages(msg_list):
        total_tokens = 0
        for msg in msg_list:
            try:
                total_tokens += count_tokens(msg["content"])
            except Exception:
                total_tokens += max(1, len(msg["content"]) // 4)
        return total_tokens

    current_tokens = count_tokens_for_messages(messages)
    if current_tokens <= max_tokens:
        return messages

    logger.info(f"è¾“å…¥tokenæ•°({current_tokens})è¶…è¿‡é™åˆ¶({max_tokens})ï¼Œè¿›è¡Œæ™ºèƒ½æˆªæ–­")
    truncated_messages = messages.copy()

    while count_tokens_for_messages(truncated_messages) > max_tokens and len(truncated_messages) > 1:
        if truncated_messages[0].get("role") == "system":
            if len(truncated_messages) > 2:
                truncated_messages.pop(1)
            else:
                break
        else:
            truncated_messages.pop(0)

    if count_tokens_for_messages(truncated_messages) > max_tokens:
        for i, msg in enumerate(truncated_messages):
            if len(msg["content"]) > 500:
                truncated_messages[i]["content"] = msg["content"][:500] + "...(å·²æˆªæ–­)"
                if count_tokens_for_messages(truncated_messages) <= max_tokens:
                    break

    final_tokens = count_tokens_for_messages(truncated_messages)
    logger.info(f"æˆªæ–­åtokenæ•°: {final_tokens}, ä¿ç•™äº† {len(truncated_messages)} æ¡æ¶ˆæ¯")
    return truncated_messages

# å¯¹è¯ç”Ÿæˆï¼šæµå¼ï¼Œä½¿ç”¨å±€éƒ¨å¼•ç”¨ä»¥é¿å…å¹¶å‘åœ¨ç”ŸæˆæœŸé—´å¸è½½æ¨¡å‹
def chat_stream(message, conversation_id):
    # ä¿æŠ¤æ¨¡å‹è¯»å–ï¼Œæ‹¿åˆ°æœ¬åœ°å¼•ç”¨
    with state.model_lock:
        if state.model is None or state.tokenizer is None:
            yield get_conversation_history(conversation_id) + [[message, "âš ï¸ è¯·å…ˆåŠ è½½æ¨¡å‹ï¼"]], get_stats_html(), get_system_info_html()
            return
        model_ref = state.model
        tokenizer_ref = state.tokenizer
        state.is_generating = True
        state.generation_stop_event = threading.Event()

    try:
        conversation = state.conversations[conversation_id]
        max_history = state.current_params['max_history']
        all_messages = conversation["messages"]
        messages_to_keep = min(len(all_messages), max_history * 2)
        recent_messages = all_messages[-messages_to_keep:] if messages_to_keep > 0 else []

        clean_message = truncate_text(clean_text(message))
        messages_for_model = recent_messages + [{"role": "user", "content": clean_message}]
        messages_for_model = smart_truncate_messages(messages_for_model)

        try:
            text = tokenizer_ref.apply_chat_template(
                messages_for_model,
                tokenize=False,
                add_generation_prompt=True,
                enable_thinking=getattr(tokenizer_ref, 'enable_thinking', False)
            )
        except Exception as e:
            logger.warning(f"åº”ç”¨èŠå¤©æ¨¡æ¿å¤±è´¥: {e}")
            text = "\n".join([f"{msg['role']}: {msg['content']}" for msg in messages_for_model]) + "\nassistant:"

        input_ids = tokenizer_ref.encode(text)
        max_input_length = CONFIG["max_input_tokens"] - CONFIG["reserved_tokens"]
        if len(input_ids) > max_input_length:
            logger.info(f"è¾“å…¥è¿‡é•¿({len(input_ids)} tokens)ï¼Œè¿›è¡Œæˆªæ–­")
            text = tokenizer_ref.decode(input_ids[:max_input_length])
            if not text.endswith("..."):
                text += "...(ä¸Šä¸‹æ–‡å·²æˆªæ–­)"

        model_device = model_ref.device if hasattr(model_ref, 'device') else 'cpu'
        model_inputs = tokenizer_ref([text], return_tensors="pt").to(model_device)

        streamer = TextIteratorStreamer(tokenizer_ref, skip_prompt=True, timeout=CONFIG["default_timeout"])
        generation_kwargs = dict(
            **model_inputs,
            max_new_tokens=state.current_params['max_new_tokens'],
            temperature=state.current_params['temperature'],
            top_p=state.current_params['top_p'],
            top_k=state.current_params.get('top_k', 50),
            do_sample=state.current_params.get('do_sample', True),
            pad_token_id=tokenizer_ref.eos_token_id,
            repetition_penalty=state.current_params['repetition_penalty'],
            streamer=streamer
        )

        start_time = time.time()
        thread = threading.Thread(target=model_ref.generate, kwargs=generation_kwargs)
        thread.daemon = True
        thread.start()

        generated_text = ""
        thinking_content = ""
        is_thinking = True

        for new_text in streamer:
            if state.generation_stop_event and state.generation_stop_event.is_set():
                logger.info("ç”Ÿæˆè¢«ç”¨æˆ·åœæ­¢")
                break
            generated_text += new_text
            if is_thinking and "</think>" in generated_text:
                parts = generated_text.split("</think>", 1)
                thinking_content = parts[0] + "</think>"
                generated_text = parts[1] if len(parts) > 1 else ""
                is_thinking = False

            if is_thinking:
                partial_response = f"ğŸ¤” **æ€è€ƒä¸­...**\n\n{thinking_content + generated_text}"
            else:
                if thinking_content:
                    partial_response = f"<details style='margin-bottom: 10px;'><summary>ğŸ§  æ€è€ƒè¿‡ç¨‹</summary>\n\n{thinking_content}\n\n</details>\n\n{generated_text}"
                else:
                    partial_response = generated_text

            partial_response = clean_text(partial_response)
            current_history = get_conversation_history(conversation_id) + [[clean_message, partial_response]]
            yield current_history, get_stats_html(), get_system_info_html()

        generation_time = time.time() - start_time
        if state.generation_stop_event and state.generation_stop_event.is_set():
            generated_text += "\n\n---\n*ç”Ÿæˆå·²è¢«ç”¨æˆ·åœæ­¢*"

        output_ids = tokenizer_ref.encode(generated_text)
        tokens_generated = len(output_ids)

        state.stats['total_requests'] += 1
        state.stats['total_tokens'] += tokens_generated
        state.stats['total_time'] += generation_time

        conversation["messages"].append({"role": "user", "content": clean_message})
        conversation["messages"].append({"role": "assistant", "content": generated_text})
        conversation["model_used"] = state.current_model

        if len(conversation["messages"]) == 2:
            conversation["title"] = clean_message[:20] + "..." if len(clean_message) > 20 else clean_message

        save_conversation(conversation_id)

        if thinking_content:
            final_response = f"<details style='margin-bottom: 10px;'><summary>ğŸ§  æ€è€ƒè¿‡ç¨‹</summary>\n\n{thinking_content}\n\n</details>\n\n{generated_text}"
        else:
            final_response = generated_text

        if not (state.generation_stop_event and state.generation_stop_event.is_set()):
            final_response += f"\n\n---\n*å“åº”æ—¶é—´: {generation_time:.2f}s | ç”ŸæˆToken: {tokens_generated}*"

        final_history = get_conversation_history(conversation_id)
        yield final_history, get_stats_html(), get_system_info_html()
    except Exception as e:
        logger.error(f"ç”Ÿæˆå“åº”æ—¶å‡ºé”™: {e}")
        state.stats['failed_requests'] += 1
        error_msg = f"âŒ ç”Ÿæˆå“åº”æ—¶å‡ºé”™: {str(e)}"
        yield get_conversation_history(conversation_id) + [[message, error_msg]], get_stats_html(), get_system_info_html()
    finally:
        state.is_generating = False
        state.generation_stop_event = None

def stop_generation():
    if state.generation_stop_event and state.is_generating:
        state.generation_stop_event.set()
        return "ğŸ›‘ æ­£åœ¨åœæ­¢ç”Ÿæˆ..."
    return "â„¹ï¸ æ²¡æœ‰æ­£åœ¨è¿›è¡Œçš„ç”Ÿæˆ"

# å‚æ•°ç®¡ç†
def update_params(max_new_tokens, temperature, top_p, repetition_penalty, max_history, top_k, do_sample):
    state.current_params.update({
        'max_new_tokens': max_new_tokens,
        'temperature': temperature,
        'top_p': top_p,
        'repetition_penalty': repetition_penalty,
        'max_history': max_history,
        'top_k': top_k,
        'do_sample': do_sample
    })
    return f"âœ… å‚æ•°å·²æ›´æ–° | é•¿åº¦: {max_new_tokens} | æ¸©åº¦: {temperature}"

def reset_params():
    state.current_params = state.default_params.copy()
    return (
        state.default_params['max_new_tokens'],
        state.default_params['temperature'],
        state.default_params['top_p'],
        state.default_params['repetition_penalty'],
        state.default_params['max_history'],
        state.default_params.get('top_k', 50),
        state.default_params.get('do_sample', True),
        "âœ… å‚æ•°å·²é‡ç½®ä¸ºé»˜è®¤å€¼"
    )

# æ¨¡å‹è·¯å¾„ç®¡ç†ï¼ˆè¿”å› gr.updateï¼Œä¸ç›´æ¥è¿”å› widgetï¼‰
def add_model_path(model_name, model_path):
    if model_name and model_path:
        MODEL_PATHS[model_name] = model_path
        save_models_config(MODEL_PATHS)
        return f"âœ… å·²æ·»åŠ æ¨¡å‹: {model_name}", gr.update(choices=list(MODEL_PATHS.keys()), value=model_name)
    return "âŒ æ¨¡å‹åç§°å’Œè·¯å¾„ä¸èƒ½ä¸ºç©º", gr.update(choices=list(MODEL_PATHS.keys()))

def remove_model_path(model_name):
    if model_name in MODEL_PATHS:
        del MODEL_PATHS[model_name]
        save_models_config(MODEL_PATHS)
        return f"âœ… å·²ç§»é™¤æ¨¡å‹: {model_name}", gr.update(choices=list(MODEL_PATHS.keys()), value=(list(MODEL_PATHS.keys())[0] if MODEL_PATHS else ""))
    return "âŒ æ¨¡å‹ä¸å­˜åœ¨", gr.update(choices=list(MODEL_PATHS.keys()))

def force_clean_memory():
    try:
        gc.collect()
        if torch.cuda.is_available():
            before_allocated = torch.cuda.memory_allocated() / (1024**3)
            before_reserved = torch.cuda.memory_reserved() / (1024**3)
            torch.cuda.empty_cache()
            try:
                for i in range(torch.cuda.device_count()):
                    torch.cuda.synchronize(i)
                    torch.cuda.empty_cache()
            except Exception:
                pass
            after_allocated = torch.cuda.memory_allocated() / (1024**3)
            after_reserved = torch.cuda.memory_reserved() / (1024**3)
            logger.info(f"å†…å­˜æ¸…ç†å®Œæˆ - å·²åˆ†é…: {before_allocated:.2f}GB -> {after_allocated:.2f}GB, ä¿ç•™: {before_reserved:.2f}GB -> {after_reserved:.2f}GB")
            return f"ğŸ§¹ å†…å­˜æ¸…ç†å®Œæˆï¼é‡Šæ”¾äº† {before_allocated - after_allocated:.2f}GB æ˜¾å­˜"
        else:
            return "ğŸ§¹ CPUå†…å­˜æ¸…ç†å®Œæˆ"
    except Exception as e:
        logger.error(f"å†…å­˜æ¸…ç†å¤±è´¥: {e}")
        return f"âŒ å†…å­˜æ¸…ç†å¤±è´¥: {str(e)}"

def initialize_app():
    try:
        load_conversations()
        config = load_config()
        if not state.conversations:
            create_new_conversation()
        else:
            state.current_conversation_id = config.get("last_conversation") or list(state.conversations.keys())[0]
        last_model = config.get("last_model")
        if last_model and last_model in MODEL_PATHS:
            logger.info(f"è‡ªåŠ¨åŠ è½½ä¸Šæ¬¡ä½¿ç”¨çš„æ¨¡å‹: {last_model}")
            load_model(MODEL_PATHS[last_model], last_model)
        logger.info("åº”ç”¨åˆå§‹åŒ–å®Œæˆ")
    except Exception as e:
        logger.error(f"åº”ç”¨åˆå§‹åŒ–å¤±è´¥: {e}")

# Helper functions for UI
def get_conversation_dropdown_options():
    options = []
    for conv_id, conversation in sorted(
        state.conversations.items(),
        key=lambda x: x[1].get("updated_at", ""),
        reverse=True
    ):
        title = conversation["title"]
        message_count = len(conversation["messages"]) // 2
        display_text = f"{title} ({message_count}æ¡æ¶ˆæ¯)"
        options.append((display_text, conv_id))
    return options

def get_current_conversation_info():
    if state.current_conversation_id in state.conversations:
        conv = state.conversations[state.current_conversation_id]
        model_info = f" | æ¨¡å‹: {conv.get('model_used', 'æœªè®°å½•')}" if conv.get('model_used') else ""
        return f"### ğŸ“ å½“å‰å¯¹è¯: {conv['title']} ({len(conv['messages'])//2} æ¡æ¶ˆæ¯{model_info})"
    return "### ğŸ“ å½“å‰å¯¹è¯: æ— "

# åˆ›å»º Gradio UI
def create_interface():
    with gr.Blocks(
        theme=gr.themes.Soft(primary_hue="blue", secondary_hue="slate"),
        title="å¤§æ¨¡å‹å¯¹è¯ç³»ç»Ÿ",
        css="""
        .gradio-container {
            max-width: 95% !important;
        }
        .conversation-item { padding: 8px 12px; margin: 2px 0; border-radius: 6px; cursor: pointer; }
        .conversation-item:hover { background: rgba(0,0,0,0.05); }
        .conversation-active { background: rgba(59, 130, 246, 0.1); border-left: 3px solid #3b82f6; }
        .warning-text { color: #e74c3c; font-size: 0.9em; }
        .success-text { color: #27ae60; font-size: 0.9em; }
        """
    ) as demo:
        gr.Markdown("# ğŸ¤– å¤§æ¨¡å‹å¯¹è¯ç³»ç»Ÿ\n**åŸºäº Qwen ç³»åˆ—æ¨¡å‹çš„æ™ºèƒ½å¯¹è¯å¹³å°**")

        cuda_status = check_cuda_availability()
        if cuda_status['available']:
            gr.Markdown(f"### ğŸ® GPUçŠ¶æ€: âœ… å¯ç”¨ - {cuda_status.get('device_name')} ({cuda_status['device_count']}ä¸ªGPU)")
        else:
            gr.Markdown("### ğŸ® GPUçŠ¶æ€: âŒ ä¸å¯ç”¨ - å°†ä½¿ç”¨CPUè¿è¡Œ")

        conversation_options = get_conversation_dropdown_options()
        conversation_choices = conversation_options  # list of (label, value)
        current_conversation_value = state.current_conversation_id or (conversation_choices[0][1] if conversation_choices else "")

        with gr.Row(equal_height=False):
            with gr.Column(scale=1, min_width=300):
                with gr.Group():
                    gr.Markdown("### ğŸ’¬ å¯¹è¯ç®¡ç†")
                    with gr.Row():
                        new_convo_btn = gr.Button("ğŸ†• æ–°å»ºå¯¹è¯", variant="primary", size="sm")
                        refresh_convos_btn = gr.Button("ğŸ”„ åˆ·æ–°", variant="secondary", size="sm")
                    conversation_dropdown = gr.Dropdown(
                        choices=conversation_choices,
                        value=current_conversation_value,
                        label="é€‰æ‹©å¯¹è¯",
                        interactive=True,
                        filterable=True
                    )
                    conversation_state = gr.State(state.current_conversation_id)
                    with gr.Row():
                        delete_convo_btn = gr.Button("ğŸ—‘ï¸ åˆ é™¤å¯¹è¯", variant="stop", size="sm")
                        export_convo_btn = gr.Button("ğŸ“¤ å¯¼å‡ºå¯¹è¯", variant="secondary", size="sm")

                with gr.Group():
                    gr.Markdown("### ğŸš€ æ¨¡å‹æ§åˆ¶")
                    model_dropdown = gr.Dropdown(
                        choices=list(MODEL_PATHS.keys()),
                        value=state.current_model or (list(MODEL_PATHS.keys())[0] if MODEL_PATHS else ""),
                        label="é€‰æ‹©æ¨¡å‹",
                        filterable=True
                    )
                    with gr.Row():
                        load_btn = gr.Button("ğŸ”„ åŠ è½½æ¨¡å‹", variant="primary", scale=1)
                        unload_btn = gr.Button("ğŸ—‘ï¸ å¸è½½æ¨¡å‹", variant="secondary", scale=1)
                    load_status = gr.Markdown("ğŸ‘† è¯·é€‰æ‹©å¹¶åŠ è½½æ¨¡å‹")
                    model_info_html = gr.HTML()

                    with gr.Accordion("ğŸ”§ æ¨¡å‹ç®¡ç†", open=False):
                        with gr.Row():
                            new_model_name = gr.Textbox(label="æ¨¡å‹åç§°", placeholder="è¾“å…¥æ¨¡å‹æ˜¾ç¤ºåç§°")
                            new_model_path = gr.Textbox(label="æ¨¡å‹è·¯å¾„", placeholder="è¾“å…¥æ¨¡å‹æœ¬åœ°è·¯å¾„")
                        with gr.Row():
                            add_model_btn = gr.Button("â• æ·»åŠ æ¨¡å‹", size="sm")
                            remove_model_btn = gr.Button("â– ç§»é™¤æ¨¡å‹", size="sm")

                with gr.Group():
                    gr.Markdown("### âš™ï¸ ç”Ÿæˆå‚æ•°")
                    with gr.Row():
                        max_new_tokens = gr.Slider(512, 8192, value=state.current_params['max_new_tokens'], step=256, label="ç”Ÿæˆé•¿åº¦")
                        temperature = gr.Slider(0.1, 2.0, value=state.current_params['temperature'], step=0.1, label="æ¸©åº¦")
                    with gr.Row():
                        top_p = gr.Slider(0.1, 1.0, value=state.current_params['top_p'], step=0.1, label="Top-P")
                        top_k = gr.Slider(1, 100, value=state.current_params.get('top_k', 50), step=1, label="Top-K")
                    with gr.Row():
                        repetition_penalty = gr.Slider(1.0, 2.0, value=state.current_params['repetition_penalty'], step=0.1, label="é‡å¤æƒ©ç½š")
                        max_history_slider = gr.Slider(1, 50, value=state.current_params['max_history'], step=1, label="å¯¹è¯è®°å¿†è½®æ•°")
                    do_sample = gr.Checkbox(value=state.current_params.get('do_sample', True), label="éšæœºé‡‡æ ·")
                    with gr.Row():
                        update_btn = gr.Button("ğŸ’¾ ä¿å­˜å‚æ•°", variant="primary", scale=1)
                        reset_btn = gr.Button("ğŸ”„ é‡ç½®é»˜è®¤", variant="secondary", scale=1)
                    param_status = gr.Markdown("âœ… å‚æ•°å·²å°±ç»ª")

                with gr.Group():
                    gr.Markdown("### ğŸ“Š ç³»ç»ŸçŠ¶æ€")
                    with gr.Row():
                        refresh_sys_btn = gr.Button("ğŸ”„ åˆ·æ–°çŠ¶æ€", variant="secondary", size="sm")
                        stop_btn = gr.Button("â¹ï¸ åœæ­¢ç”Ÿæˆ", variant="stop", size="sm")
                        clean_memory_btn = gr.Button("ğŸ§¹ æ¸…ç†å†…å­˜", variant="secondary", size="sm")
                    system_info_html = gr.HTML(get_system_info_html())
                    stats_html = gr.HTML(get_stats_html())

            with gr.Column(scale=2, min_width=500):
                current_conversation_display = gr.Markdown(get_current_conversation_info())
                chatbot = gr.Chatbot(
                    value=get_conversation_history(state.current_conversation_id),
                    label="ğŸ’¬ æ™ºèƒ½å¯¹è¯",
                    height=500,
                    type="tuples",
                    show_copy_button=True,
                    avatar_images=(
                        "https://cdn-icons-png.flaticon.com/512/149/149071.png",
                        "https://cdn-icons-png.flaticon.com/512/4712/4712035.png"
                    ),
                    placeholder="å¯¹è¯è®°å½•å°†æ˜¾ç¤ºåœ¨è¿™é‡Œ..."
                )
                with gr.Row():
                    msg = gr.Textbox(
                        label="",
                        placeholder="è¯·è¾“å…¥æ‚¨çš„é—®é¢˜æˆ–æŒ‡ä»¤...ï¼ˆæŒ‰Enterå‘é€ï¼ŒShift+Enteræ¢è¡Œï¼‰",
                        lines=2,
                        max_lines=5,
                        scale=4,
                        show_label=False
                    )
                    submit_btn = gr.Button("ğŸš€ å‘é€", variant="primary", scale=1)

                with gr.Row():
                    gr.Markdown("**ğŸ’¡ å¿«æ·æ“ä½œ:**")
                    quick_clear = gr.Button("ğŸ—‘ï¸ æ¸…ç©ºå½“å‰", size="sm")
                    quick_example1 = gr.Button("ğŸ‘‹ æ‰“ä¸ªæ‹›å‘¼", size="sm")
                    quick_example2 = gr.Button("ğŸ“ å†™æ®µä»£ç ", size="sm")
                    quick_example3 = gr.Button("ğŸ¤” è§£é‡Šæ¦‚å¿µ", size="sm")

                message_counter = gr.HTML(f"""
                <div style="text-align: right; font-size: 0.8em; color: #666; margin-top: 5px;">
                    å½“å‰å¯¹è¯: {len(state.conversations.get(state.current_conversation_id, {}).get('messages', []))//2} æ¡æ¶ˆæ¯
                </div>
                """)

        # äº‹ä»¶å¤„ç†ï¼ˆæ‰€æœ‰handlerè¿”å›åŸºç¡€ç±»å‹æˆ– gr.updateï¼‰
        def handle_load_model(model_name):
            if model_name in MODEL_PATHS:
                model_path = MODEL_PATHS[model_name]
                load_result, model_html = load_model(model_path, model_name)
                return load_result, model_html, get_stats_html(), get_system_info_html()
            return "âŒ è¯·é€‰æ‹©æœ‰æ•ˆçš„æ¨¡å‹", "", get_stats_html(), get_system_info_html()

        def handle_unload_model():
            unload_result, model_html = unload_model()
            return unload_result, model_html, get_stats_html(), get_system_info_html()

        def handle_submit(message, conversation_id):
            if not message.strip():
                yield get_conversation_history(conversation_id), get_stats_html(), get_system_info_html(), "", get_current_conversation_info()
                return
            for updated_history, stats_html_content, system_html_content in chat_stream(message, conversation_id):
                yield updated_history, stats_html_content, system_html_content, "", get_current_conversation_info()

        def handle_new_conversation():
            new_id = create_new_conversation()
            config = load_config()
            config["last_conversation"] = new_id
            save_config(config)
            options = get_conversation_dropdown_options()
            new_conv_title = f"æ–°å¯¹è¯ (0æ¡æ¶ˆæ¯)"
            return (
                gr.update(choices=options, value=new_id),
                get_conversation_history(new_id),
                get_current_conversation_info(),
                new_id
            )

        def handle_refresh_conversations():
            load_conversations()
            options = get_conversation_dropdown_options()
            current_value = state.current_conversation_id or (options[0][1] if options else "")
            return gr.update(choices=options, value=current_value)

        def handle_conversation_change(selected_conv_id):
            if selected_conv_id:
                if selected_conv_id in state.conversations:
                    state.current_conversation_id = selected_conv_id
                    config = load_config()
                    config["last_conversation"] = selected_conv_id
                    save_config(config)
                    return (
                        get_conversation_history(selected_conv_id),
                        get_current_conversation_info(),
                        selected_conv_id
                    )
            return gr.update(), gr.update(), state.current_conversation_id

        def handle_delete_conversation(conv_id):
            if conv_id:
                success = delete_conversation(conv_id)
                if success:
                    options = get_conversation_dropdown_options()
                    current_value = state.current_conversation_id or (options[0][1] if options else "")
                    history = get_conversation_history(state.current_conversation_id) if state.current_conversation_id else []
                    return (
                        gr.update(choices=options, value=current_value),
                        history,
                        get_current_conversation_info(),
                        state.current_conversation_id,
                        "âœ… å¯¹è¯å·²åˆ é™¤"
                    )
            return gr.update(), gr.update(), gr.update(), gr.update(), "âŒ åˆ é™¤å¯¹è¯å¤±è´¥"

        def handle_export_conversation(conv_id):
            if conv_id in state.conversations:
                conversation = state.conversations[conv_id]
                export_data = {
                    "title": conversation["title"],
                    "model_used": conversation.get("model_used", "æœªçŸ¥"),
                    "created_at": conversation["created_at"],
                    "messages": conversation["messages"]
                }
                export_str = json.dumps(export_data, ensure_ascii=False, indent=2)
                tmp = tempfile.NamedTemporaryFile(delete=False, suffix=".json", prefix="export_", mode="w", encoding="utf-8")
                tmp.write(export_str)
                tmp.flush()
                tmp.close()
                filename = f"{conversation['title']}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
                return gr.update(value=tmp.name), filename, "âœ… å¯¼å‡ºæˆåŠŸ"
            return gr.update(value=None), "", "âŒ å¯¼å‡ºå¤±è´¥"

        def handle_clear_current_conversation(conv_id):
            if conv_id in state.conversations:
                state.conversations[conv_id]["messages"] = []
                save_conversation(conv_id)
                return [], get_current_conversation_info()
            return [], get_current_conversation_info()

        def handle_quick_example(example_type):
            examples = {
                "greeting": "ä½ å¥½ï¼è¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚",
                "code": "è¯·ç”¨Pythonå†™ä¸€ä¸ªå¿«é€Ÿæ’åºç®—æ³•ï¼Œå¹¶æ·»åŠ è¯¦ç»†æ³¨é‡Šã€‚",
                "explain": "è¯·ç”¨é€šä¿—æ˜“æ‡‚çš„æ–¹å¼è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ã€‚"
            }
            return examples.get(example_type, "ä½ å¥½ï¼")

        # ç»‘å®šäº‹ä»¶
        load_btn.click(
            fn=handle_load_model,
            inputs=[model_dropdown],
            outputs=[load_status, model_info_html, stats_html, system_info_html]
        )
        unload_btn.click(
            fn=handle_unload_model,
            outputs=[load_status, model_info_html, stats_html, system_info_html]
        )
        submit_event = msg.submit(
            fn=handle_submit,
            inputs=[msg, conversation_state],
            outputs=[chatbot, stats_html, system_info_html, msg, current_conversation_display]
        )
        submit_btn.click(
            fn=handle_submit,
            inputs=[msg, conversation_state],
            outputs=[chatbot, stats_html, system_info_html, msg, current_conversation_display]
        )
        stop_btn.click(fn=stop_generation, outputs=[load_status])
        clean_memory_btn.click(fn=force_clean_memory, outputs=[load_status])
        new_convo_btn.click(fn=handle_new_conversation, outputs=[conversation_dropdown, chatbot, current_conversation_display, conversation_state])
        refresh_convos_btn.click(fn=handle_refresh_conversations, outputs=[conversation_dropdown])
        # åˆ é™¤å¯¹è¯ç°åœ¨ä¼ å…¥å¯¹è¯ ID
        delete_convo_btn.click(fn=lambda: handle_delete_conversation(state.current_conversation_id), outputs=[conversation_dropdown, chatbot, current_conversation_display, conversation_state, load_status])
        export_convo_btn.click(fn=lambda: handle_export_conversation(state.current_conversation_id), outputs=[gr.File(), gr.Textbox(), load_status])
        quick_clear.click(fn=lambda: handle_clear_current_conversation(state.current_conversation_id), outputs=[chatbot, current_conversation_display])
        quick_example1.click(fn=lambda: handle_quick_example("greeting"), outputs=[msg])
        quick_example2.click(fn=lambda: handle_quick_example("code"), outputs=[msg])
        quick_example3.click(fn=lambda: handle_quick_example("explain"), outputs=[msg])
        refresh_sys_btn.click(fn=lambda: [get_system_info_html(), get_stats_html()], outputs=[system_info_html, stats_html])
        add_model_btn.click(fn=add_model_path, inputs=[new_model_name, new_model_path], outputs=[load_status, model_dropdown])
        remove_model_btn.click(fn=remove_model_path, inputs=[model_dropdown], outputs=[load_status, model_dropdown])
        update_btn.click(fn=update_params, inputs=[max_new_tokens, temperature, top_p, repetition_penalty, max_history_slider, top_k, do_sample], outputs=[param_status])
        reset_btn.click(fn=reset_params, outputs=[max_new_tokens, temperature, top_p, repetition_penalty, max_history_slider, top_k, do_sample, param_status])

        return demo

if __name__ == "__main__":
    initialize_app()
    demo = create_interface()
    try:
        demo.queue(max_size=20, api_open=False).launch(
            server_name="0.0.0.0",
            server_port=7860,
            share=False,
            inbrowser=True,
            show_error=True,
            quiet=False,
            debug=False
        )
    except Exception as e:
        logger.error(f"å¯åŠ¨åº”ç”¨å¤±è´¥: {e}")
        print(f"å¯åŠ¨å¤±è´¥: {e}")